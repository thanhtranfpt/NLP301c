{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb49525",
   "metadata": {},
   "source": [
    "# Bài 1: Dịch máy thần kinh\n",
    "\n",
    "Chào mừng bạn đến với bài tập đầu tiên của Khóa 4. Tại đây, bạn sẽ xây dựng mô hình dịch máy thần kinh từ tiếng Anh sang tiếng Bồ Đào Nha (NMT) bằng cách sử dụng mạng Bộ nhớ ngắn hạn dài (LSTM) một cách chú ý. Dịch máy là một nhiệm vụ quan trọng trong xử lý ngôn ngữ tự nhiên và có thể hữu ích không chỉ trong việc dịch ngôn ngữ này sang ngôn ngữ khác mà còn giúp phân biệt nghĩa của từ (ví dụ: xác định xem từ \"ngân hàng\" ám chỉ ngân hàng tài chính hay vùng đất ven sông) . Việc triển khai điều này chỉ bằng cách sử dụng Mạng thần kinh tái phát (RNN) với LSTM có thể hoạt động đối với các câu có độ dài từ ngắn đến trung bình nhưng có thể dẫn đến biến mất độ dốc đối với các chuỗi rất dài. Để giải quyết vấn đề này, bạn sẽ thêm một cơ chế chú ý để cho phép bộ giải mã truy cập tất cả các phần có liên quan của câu đầu vào bất kể độ dài của nó. Khi hoàn thành nhiệm vụ này, bạn sẽ:\n",
    "\n",
    "- Triển khai hệ thống mã hóa-giải mã một cách chu đáo\n",
    "- Xây dựng mô hình NMT từ đầu bằng Tensorflow\n",
    "- Tạo bản dịch bằng cách sử dụng giải mã Tham lam và Rủi ro Bayes Tối thiểu (MBR)\n",
    "\n",
    "## Mục lục\n",
    "- [1 - Data Preparation](#1)\n",
    "- [2 - NMT model with attention](#2)\n",
    "- [Exercise 1 - Encoder](#ex1)\n",
    "- [Exercise 2 - CrossAttention](#ex2)\n",
    "- [Exercise 3 - Decoder](#ex3)\n",
    "- [Exercise 4 - Translator](#ex4)\n",
    "- [3 - Training](#3)\n",
    "- [4 - Using the model for inference ](#4)\n",
    "- [Exercise 5 - translate](#ex5)\n",
    "- [5 - Minimum Bayes-Risk Decoding](#5)\n",
    "- [Exercise 6 - rouge1_similarity](#ex6)\n",
    "- [Exercise 7 - average_overlap](#ex7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ef370d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from utils import (sentences, train_data, val_data, english_vectorizer, portuguese_vectorizer, \n",
    "                   masked_loss, masked_acc, tokens_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adb8fd6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76be1dc",
   "metadata": {},
   "source": [
    "<a tên=\"1\"></a>\n",
    "## 1. Chuẩn bị dữ liệu\n",
    "\n",
    "Các bit xử lý trước văn bản đã được xử lý (nếu bạn quan tâm đến vấn đề này, hãy nhớ kiểm tra tệp `utils.py`). Các bước thực hiện có thể tóm tắt như sau:\n",
    "\n",
    "- Đọc dữ liệu thô từ file văn bản\n",
    "- Làm sạch dữ liệu (sử dụng chữ thường, thêm khoảng trắng xung quanh dấu câu, cắt bớt khoảng trắng, v.v.)\n",
    "- Chia nó thành tập huấn luyện và xác nhận\n",
    "- Thêm dấu đầu câu và dấu cuối câu vào mỗi câu\n",
    "- Token hóa các câu\n",
    "- Tạo tập dữ liệu Tensorflow từ các câu được mã hóa\n",
    "\n",
    "Hãy dành chút thời gian để kiểm tra các câu thô:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226033a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (to translate) sentence:\n",
      "\n",
      "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
      "\n",
      "Portuguese (translation) sentence:\n",
      "\n",
      "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
     ]
    }
   ],
   "source": [
    "portuguese_sentences, english_sentences = sentences\n",
    "\n",
    "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
    "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba90eb9",
   "metadata": {},
   "source": [
    "Các câu thô bạn không có nhiều sử dụng nên hãy xóa chúng đi để tiết kiệm bộ nhớ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f081b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "del portuguese_sentences\n",
    "del english_sentences\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff83d2",
   "metadata": {},
   "source": [
    "Lưu ý rằng bạn đã nhập `english_vectorizer` và `tiếng Bồ Đào Nha_vectorizer` từ `utils.py`. Chúng được tạo bằng [tf.keras.layers.TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) và chúng cung cấp các tính năng thú vị như cách trực quan hóa từ vựng và chuyển đổi văn bản thành id được mã hóa và ngược lại. Trên thực tế, bạn có thể kiểm tra mười từ đầu tiên của từ vựng cho cả hai ngôn ngữ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1cfc17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words of the english vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n",
      "\n",
      "First 10 words of the portuguese vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
    "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152b075",
   "metadata": {},
   "source": [
    "Lưu ý rằng 4 từ đầu tiên được dành riêng cho những từ đặc biệt. Theo thứ tự, đó là:\n",
    "\n",
    "- chuỗi trống\n",
    "- một mã thông báo đặc biệt để đại diện cho một từ chưa biết\n",
    "- một mã thông báo đặc biệt để thể hiện sự bắt đầu của một câu\n",
    "- một mã thông báo đặc biệt để thể hiện sự kết thúc của câu\n",
    "\n",
    "Bạn có thể xem có bao nhiêu từ trong từ vựng bằng cách sử dụng phương thức `vocabulary_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5facaa0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese vocabulary is made up of 12000 words\n",
      "English vocabulary is made up of 12000 words\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary\n",
    "vocab_size_por = portuguese_vectorizer.vocabulary_size()\n",
    "vocab_size_eng = english_vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\n",
    "print(f\"English vocabulary is made up of {vocab_size_eng} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4b615",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Bạn có thể xác định các đối tượng [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) sẽ giúp bạn ánh xạ từ từ sang id và ngược lại. Thực hiện điều này đối với từ vựng tiếng Bồ Đào Nha vì điều này sẽ hữu ích sau này khi bạn giải mã các dự đoán từ mô hình của mình:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218f7a36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# This helps you convert from words to ids\n",
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(), \n",
    "    mask_token=\"\", \n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "# This helps you convert from ids to words\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\",\n",
    "    invert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af8b623",
   "metadata": {},
   "source": [
    "Hãy dùng thử để tìm các mã thông báo đặc biệt và một từ ngẫu nhiên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20076b9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id for the [UNK] token is 1\n",
      "The id for the [SOS] token is 2\n",
      "The id for the [EOS] token is 3\n",
      "The id for baunilha (vanilla) is 7079\n"
     ]
    }
   ],
   "source": [
    "unk_id = word_to_id(\"[UNK]\")\n",
    "sos_id = word_to_id(\"[SOS]\")\n",
    "eos_id = word_to_id(\"[EOS]\")\n",
    "baunilha_id = word_to_id(\"baunilha\")\n",
    "\n",
    "print(f\"The id for the [UNK] token is {unk_id}\")\n",
    "print(f\"The id for the [SOS] token is {sos_id}\")\n",
    "print(f\"The id for the [EOS] token is {eos_id}\")\n",
    "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d744c",
   "metadata": {},
   "source": [
    "Cuối cùng hãy xem dữ liệu sẽ được đưa vào mạng lưới thần kinh trông như thế nào. Cả `train_data` và `val_data` đều thuộc loại `tf.data.Dataset` và đã được sắp xếp theo lô gồm 64 ví dụ. Để lấy lô đầu tiên ra khỏi tập dữ liệu tf, bạn có thể sử dụng phương thức `take`. Để lấy ví dụ đầu tiên trong lô, bạn có thể cắt tensor và sử dụng phương thức `numpy` để in đẹp hơn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739777eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized english sentence:\n",
      "[   2  210    9  146  123   38    9 1672    4    3    0    0    0    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence (shifted to the right):\n",
      "[   2 1085    7  128   11  389   37 2038    4    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence:\n",
      "[1085    7  128   11  389   37 2038    4    3    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (to_translate, sr_translation), translation in train_data.take(1):\n",
    "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9ee3c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Có một vài chi tiết quan trọng cần chú ý.\n",
    "\n",
    "- Phần đệm đã được áp dụng cho các tensor và giá trị được sử dụng cho việc này là 0\n",
    "- Mỗi ví dụ gồm 3 tensor khác nhau:\n",
    "- Câu cần dịch\n",
    "- Dịch chuyển sang phải\n",
    "- Bản dịch\n",
    "\n",
    "Hai cái đầu tiên có thể được coi là các tính năng, trong khi cái thứ ba là mục tiêu. Bằng cách này, mô hình của bạn có thể thực hiện Buộc giáo viên như bạn đã thấy trong các bài giảng.\n",
    "\n",
    "Bây giờ là lúc để bắt đầu viết mã!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41cb52",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a tên=\"2\"></a>\n",
    "##2. Mô hình NMT có sự chú ý\n",
    "\n",
    "Mô hình bạn sẽ xây dựng sử dụng kiến ​​trúc bộ mã hóa-giải mã. Mạng thần kinh tái phát (RNN) này lấy phiên bản mã hóa của một câu trong bộ mã hóa của nó, sau đó chuyển nó đến bộ giải mã để dịch. Như đã đề cập trong các bài giảng, chỉ cần sử dụng mô hình tuần tự thông thường với LSTM sẽ hoạt động hiệu quả đối với các câu ngắn và trung bình nhưng sẽ bắt đầu kém hiệu quả đối với các câu dài hơn. Bạn có thể hình dung nó giống như hình bên dưới, trong đó tất cả ngữ cảnh của câu đầu vào được nén thành một vectơ được chuyển vào khối giải mã. Bạn có thể thấy đây sẽ là vấn đề như thế nào đối với các câu rất dài (ví dụ: 100 mã thông báo trở lên) vì ngữ cảnh của phần đầu tiên của dữ liệu đầu vào sẽ có rất ít ảnh hưởng đến vectơ cuối cùng được truyền tới bộ giải mã.\n",
    "\n",
    "<img src='images/plain_rnn.png'>\n",
    "\n",
    "Việc thêm lớp chú ý vào mô hình này sẽ tránh được vấn đề này bằng cách cấp cho bộ giải mã quyền truy cập vào tất cả các phần của câu đầu vào. Để minh họa, chúng ta hãy sử dụng câu đầu vào gồm 4 từ như dưới đây. Hãy nhớ rằng trạng thái ẩn được tạo ra ở mỗi dấu thời gian của bộ mã hóa (được biểu thị bằng các hình chữ nhật màu cam). Tất cả đều được chuyển đến lớp chú ý và mỗi điểm được cho điểm dựa trên mức kích hoạt hiện tại (tức là trạng thái ẩn) của bộ giải mã. Ví dụ: hãy xem hình bên dưới nơi dự đoán đầu tiên \"como\" đã được đưa ra. Để đưa ra dự đoán tiếp theo, lớp chú ý trước tiên sẽ nhận tất cả các trạng thái ẩn của bộ mã hóa (tức là hình chữ nhật màu cam) cũng như trạng thái ẩn của bộ giải mã khi tạo ra từ \"como\" (tức là hình chữ nhật màu xanh lá cây đầu tiên). Dựa trên thông tin này, nó sẽ chấm điểm từng trạng thái ẩn của bộ mã hóa để biết bộ giải mã nên tập trung vào trạng thái nào để tạo ra từ tiếp theo. Sau quá trình đào tạo, mô hình có thể đã học được rằng nó phải căn chỉnh theo trạng thái ẩn của bộ mã hóa thứ hai và sau đó gán xác suất cao cho từ \"você\". Nếu chúng ta đang sử dụng giải mã tham lam, chúng ta sẽ xuất từ ​​đã nói dưới dạng ký hiệu tiếp theo, sau đó khởi động lại quá trình để tạo ra từ tiếp theo cho đến khi đạt được dự đoán ở cuối câu.\n",
    "\n",
    "<img src='images/attention_overview.png'>\n",
    "\n",
    "\n",
    "Có nhiều cách khác nhau để triển khai sự chú ý và cách chúng tôi sẽ sử dụng cho bài tập này là Chú ý sản phẩm chấm theo tỷ lệ có dạng:\n",
    "\n",
    "$$Chú ý(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Bạn sẽ tìm hiểu sâu hơn về phương trình này trong tuần tới nhưng hiện tại, bạn có thể coi nó như cách tính điểm bằng cách sử dụng truy vấn (Q) và khóa (K), sau đó là phép nhân các giá trị (V) để có được vectơ ngữ cảnh tại một dấu thời gian cụ thể của bộ giải mã. Vectơ ngữ cảnh này được đưa đến bộ giải mã RNN để nhận tập hợp xác suất cho từ được dự đoán tiếp theo. Phép chia theo căn bậc hai của chiều khóa ($\\sqrt{d_k}$) là để cải thiện hiệu suất mô hình và bạn cũng sẽ tìm hiểu thêm về nó vào tuần tới. Đối với ứng dụng dịch máy của chúng tôi, kích hoạt bộ mã hóa (tức là trạng thái ẩn của bộ mã hóa) sẽ là khóa và giá trị, trong khi kích hoạt bộ giải mã (tức là trạng thái ẩn của bộ giải mã) sẽ là truy vấn.\n",
    "\n",
    "Bạn sẽ thấy trong các phần sắp tới rằng kiến ​​trúc và cơ chế phức tạp này có thể được triển khai chỉ với một vài dòng mã.\n",
    "\n",
    "Trước tiên, bạn sẽ xác định hai biến toàn cục quan trọng:\n",
    "\n",
    "- Kích thước của từ vựng\n",
    "- Số lượng đơn vị trong các lớp LSTM (số lượng giống nhau sẽ được sử dụng cho tất cả các lớp LSTM)\n",
    "\n",
    "Trong bài tập này, kích thước từ vựng của tiếng Anh và tiếng Bồ Đào Nha là như nhau. Do đó, chúng tôi sử dụng một hằng số VOCAB_SIZE trong toàn bộ sổ ghi chép. Mặc dù ở các bối cảnh khác, kích thước từ vựng có thể khác nhau nhưng điều đó không xảy ra trong bài tập của chúng tôi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e484abf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc251965",
   "metadata": {},
   "source": [
    "<a name=\"ex1\"></a>\n",
    "## Bài tập 1 - Bộ mã hóa\n",
    "\n",
    "Bài tập đầu tiên của bạn là mã hóa phần mã hóa của mạng lưới thần kinh. Để làm được điều này, hãy hoàn thành lớp `Bộ mã hóa` bên dưới. Lưu ý rằng trong hàm tạo (phương thức `__init__`), bạn cần xác định tất cả các lớp con của bộ mã hóa và sau đó sử dụng các lớp con này trong quá trình chuyển tiếp (phương thức `call`).\n",
    "\n",
    "Bộ mã hóa bao gồm các lớp sau:\n",
    "\n",
    "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). Đối với lớp này, bạn cần xác định `input_dim` và `output_dim` thích hợp và cho nó biết rằng bạn đang sử dụng '0' làm phần đệm, điều này có thể được thực hiện bằng cách sử dụng giá trị thích hợp cho tham số `mask_zero`.\n",
    "\n",
    "+ [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional). Trong TF, bạn có thể triển khai hành vi hai chiều cho các lớp giống RNN. Phần này đã được xử lý nhưng bạn sẽ cần chỉ định loại lớp thích hợp cũng như các tham số của nó. Đặc biệt, bạn cần đặt số lượng đơn vị thích hợp và đảm bảo rằng LSTM trả về chuỗi đầy đủ chứ không chỉ đầu ra cuối cùng, điều này có thể được thực hiện bằng cách sử dụng giá trị thích hợp cho tham số `return_sequences`.\n",
    "\n",
    "\n",
    "Bạn cần xác định chuyển tiếp bằng cú pháp [functional API](https://www.tensorflow.org/guide/keras/functional_api) của TF. Điều này có nghĩa là bạn xâu chuỗi các lệnh gọi hàm lại với nhau để xác định mạng của mình như thế này:\n",
    "\n",
    "```python\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1db0a1d",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: Encoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(  \n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.rnn = tf.keras.layers.Bidirectional(  \n",
    "            merge_mode=\"sum\",  \n",
    "            layer=tf.keras.layers.LSTM(\n",
    "                units=units,\n",
    "                return_sequences=True\n",
    "            ),  \n",
    "        )  \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, context):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): The sentence to translate\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Encoded sentence to translate\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Pass the context through the embedding layer\n",
    "        x = self.embedding(context)\n",
    "\n",
    "        # Pass the output of the embedding through the RNN\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65034ffd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences in english has shape: (64, 14)\n",
      "\n",
      "Encoder output has shape: (64, 14, 256)\n"
     ]
    }
   ],
   "source": [
    "# Do a quick check of your implementation\n",
    "\n",
    "# Create an instance of your class\n",
    "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# Pass a batch of sentences to translate from english to portuguese\n",
    "encoder_output = encoder(to_translate)\n",
    "\n",
    "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
    "print(f'Encoder output has shape: {encoder_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909aea1",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "Tensor of sentences in english has shape: (64, 14)\n",
    "\n",
    "Encoder output has shape: (64, 14, 256)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3031bb14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "\n",
    "w1_unittest.test_encoder(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe83f4",
   "metadata": {},
   "source": [
    "<a name=\"ex2\"></a>\n",
    "## Bài tập 2 - CrossAttention\n",
    "\n",
    "Bài tập tiếp theo của bạn là mã hóa lớp sẽ thực hiện chú ý chéo giữa các câu gốc và bản dịch. Để làm được điều này, hãy hoàn thành lớp `CrossAttention` bên dưới. Lưu ý rằng trong hàm tạo (phương thức `__init__`), bạn cần xác định tất cả các lớp con và sau đó sử dụng các lớp con này trong quá trình chuyển tiếp (phương thức `call`). Đối với trường hợp cụ thể này, một số bit này đã được xử lý.\n",
    "\n",
    "Sự chú ý chéo bao gồm các lớp sau:\n",
    "\n",
    "- [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). Đối với lớp này, bạn cần xác định `key_dim` thích hợp, đó là kích thước của các tensor khóa và truy vấn. Bạn cũng sẽ cần đặt số lượng đầu thành 1 vì bạn không triển khai sự chú ý nhiều đầu mà là sự chú ý giữa hai tensor. Lý do tại sao lớp này được ưa thích hơn [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) là vì nó cho phép mã đơn giản hơn trong quá trình chuyển tiếp.\n",
    "\n",
    "Một vài điều cần chú ý:\n",
    "- Bạn cần một cách để chuyển cả đầu ra của chú ý cùng với bản dịch dịch sang phải (vì chú ý chéo này xảy ra ở phía bộ giải mã). Đối với điều này, bạn sẽ sử dụng lớp [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add) để kích thước ban đầu được giữ nguyên, điều này sẽ không xảy ra nếu bạn sử dụng thứ gì đó giống như lớp [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add).\n",
    "\n",
    "+ Việc chuẩn hóa lớp cũng được thực hiện để mạng ổn định hơn bằng cách sử dụng lớp [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization).\n",
    "\n",
    "- Bạn không cần phải lo lắng về những bước cuối cùng này vì chúng đã được giải quyết rồi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e71f3d",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: CrossAttention\n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        self.mha = ( \n",
    "            tf.keras.layers.MultiHeadAttention(\n",
    "                key_dim=units,\n",
    "                num_heads=1\n",
    "            ) \n",
    "        )  \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, context, target):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The embedded shifted-to-the-right translation\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Cross attention between context and target\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Call the MH attention by passing in the query and value\n",
    "        # For this case the query should be the translation and the value the encoded sentence to translate\n",
    "        # Hint: Check the call arguments of MultiHeadAttention in the docs\n",
    "        attn_output = self.mha(\n",
    "            query=target,\n",
    "            value=context\n",
    "        )  \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        x = self.add([target, attn_output])\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c62796f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (64, 14, 256)\n",
      "Tensor of translations has shape: (64, 15, 256)\n",
      "Tensor of attention scores has shape: (64, 15, 256)\n"
     ]
    }
   ],
   "source": [
    "# Do a quick check of your implementation\n",
    "\n",
    "# Create an instance of your class\n",
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# The attention layer expects the embedded sr-translation and the context\n",
    "# The context (encoder_output) is already embedded so you need to do this for sr_translation:\n",
    "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
    "\n",
    "# Compute the cross attention\n",
    "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
    "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4f99a",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "Tensor of contexts has shape: (64, 14, 256)\n",
    "Tensor of translations has shape: (64, 15, 256)\n",
    "Tensor of attention scores has shape: (64, 15, 256)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f658975",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "\n",
    "w1_unittest.test_cross_attention(CrossAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa296ee2",
   "metadata": {},
   "source": [
    "<a name=\"ex3\"></a>\n",
    "## Bài tập 3 – Bộ giải mã\n",
    "\n",
    "\n",
    "Bây giờ bạn sẽ triển khai phần giải mã của mạng nơ-ron bằng cách hoàn thành lớp `Bộ giải mã` bên dưới. Lưu ý rằng trong hàm tạo (phương thức `__init__`), bạn cần xác định tất cả các lớp con của bộ giải mã và sau đó sử dụng các lớp con này trong quá trình chuyển tiếp (phương thức `call`).\n",
    "\n",
    "Bộ giải mã bao gồm các lớp sau:\n",
    "\n",
    "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). Đối với lớp này, bạn cần xác định `input_dim` và `output_dim` thích hợp và cho nó biết rằng bạn đang sử dụng '0' làm phần đệm, điều này có thể được thực hiện bằng cách sử dụng giá trị thích hợp cho tham số `mask_zero`.\n",
    "\n",
    "\n",
    "+ Chú ý trước [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Không giống như trong bộ mã hóa mà bạn đã sử dụng LSTM hai chiều, ở đây bạn sẽ sử dụng LSTM thông thường. Đừng quên đặt số lượng đơn vị thích hợp và đảm bảo rằng LSTM trả về chuỗi đầy đủ chứ không chỉ đầu ra cuối cùng, điều này có thể được thực hiện bằng cách sử dụng giá trị thích hợp cho tham số `return_sequences`. Điều rất quan trọng là lớp này trả về trạng thái vì điều này sẽ cần thiết cho suy luận, vì vậy hãy đảm bảo đặt tham số `return_state` cho phù hợp. Lưu ý rằng các lớp LSTM trả về trạng thái dưới dạng một bộ gồm hai tensor được gọi là `memory_state` và `carry_state`, **tuy nhiên, những tên này đã được thay đổi để phản ánh tốt hơn những gì bạn đã thấy trong bài giảng thành `hidden_state` và `cell_state`** .\n",
    "\n",
    "- Lớp chú ý thực hiện chú ý chéo giữa câu cần dịch và bản dịch dịch sang phải. Ở đây bạn cần sử dụng lớp `CrossAttention` mà bạn đã xác định trong bài tập trước.\n",
    "\n",
    "+ Hậu chú ý [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Một lớp LSTM khác. Đối với cái này bạn không cần nó để trả lại trạng thái.\n",
    "\n",
    "- Cuối cùng là lớp [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). Cái này phải có cùng số đơn vị với kích thước của từ vựng vì bạn mong đợi nó tính toán logit cho mọi từ có thể có trong từ vựng. Đảm bảo sử dụng chức năng kích hoạt `logsoftmax` cho chức năng này, bạn có thể nhận được chức năng này dưới dạng [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9639bdb",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: Decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # The embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        # The RNN before attention\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )  \n",
    "\n",
    "        # The attention layer\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # The RNN after attention\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True\n",
    "        )  \n",
    "\n",
    "        # The dense layer with logsoftmax activation\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=vocab_size,\n",
    "            activation=tf.nn.log_softmax\n",
    "        )  \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The shifted-to-the-right translation\n",
    "            state (list[tf.Tensor, tf.Tensor], optional): Hidden state of the pre-attention LSTM. Defaults to None.\n",
    "            return_state (bool, optional): If set to true return the hidden states of the LSTM. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Get the embedding of the input\n",
    "        x = self.embedding(target)\n",
    "\n",
    "        # Pass the embedded input into the pre attention LSTM\n",
    "        # Hints:\n",
    "        # - The LSTM you defined earlier should return the output alongside the state (made up of two tensors)\n",
    "        # - Pass in the state to the LSTM (needed for inference)\n",
    "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
    "\n",
    "        # Perform cross attention between the context and the output of the LSTM (in that order)\n",
    "        x = self.attention(context, x)\n",
    "\n",
    "        # Do a pass through the post attention LSTM\n",
    "        x = self.post_attention_rnn(x)\n",
    "\n",
    "        # Compute the logits\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6165cf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (64, 14, 256)\n",
      "Tensor of right-shifted translations has shape: (64, 15)\n",
      "Tensor of logits has shape: (64, 15, 12000)\n"
     ]
    }
   ],
   "source": [
    "# Do a quick check of your implementation\n",
    "\n",
    "# Create an instance of your class\n",
    "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# Notice that you don't need the embedded version of sr_translation since this is done inside the class\n",
    "logits = decoder(encoder_output, sr_translation)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b5d7d",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "Tensor of contexts has shape: (64, 14, 256)\n",
    "Tensor of right-shifted translations has shape: (64, 15)\n",
    "Tensor of logits has shape: (64, 15, 12000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b61093a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "\n",
    "w1_unittest.test_decoder(Decoder, CrossAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcce3a7",
   "metadata": {},
   "source": [
    "<a name=\"ex4\"></a>\n",
    "## Bài tập 4 – Trình dịch\n",
    "\n",
    "Bây giờ bạn phải tập hợp tất cả các lớp mà bạn đã mã hóa trước đó thành một mô hình thực tế. Để làm điều này, hãy hoàn thành lớp `Translator` bên dưới. Lưu ý rằng không giống như các lớp Bộ mã hóa và Bộ giải mã được kế thừa từ `tf.keras.layers.Layer`, lớp Translator kế thừa từ `tf.keras.Model`.\n",
    "\n",
    "Hãy nhớ rằng `train_data` sẽ tạo ra một bộ dữ liệu có câu cần dịch và bản dịch được dịch sang phải, đây là những \"đặc điểm\" của mô hình. Điều này có nghĩa là đầu vào của mạng của bạn sẽ là các bộ chứa ngữ cảnh và mục tiêu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "205fcf31",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: Translator\n",
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Define the encoder with the appropriate vocab_size and number of units\n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "\n",
    "        # Define the decoder with the appropriate vocab_size and number of units\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            inputs (tuple(tf.Tensor, tf.Tensor)): Tuple containing the context (sentence to translate) and the target (shifted-to-the-right translation)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # In this case inputs is a tuple consisting of the context and the target, unpack it into single variables\n",
    "        context, target = inputs\n",
    "\n",
    "        # Pass the context through the encoder\n",
    "        encoded_context = self.encoder(context)\n",
    "\n",
    "        # Compute the logits by passing the encoded context and the target to the decoder\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d4a231c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences to translate has shape: (64, 14)\n",
      "Tensor of right-shifted translations has shape: (64, 15)\n",
      "Tensor of logits has shape: (64, 15, 12000)\n"
     ]
    }
   ],
   "source": [
    "# Do a quick check of your implementation\n",
    "\n",
    "# Create an instance of your class\n",
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# Compute the logits for every word in the vocabulary\n",
    "logits = translator((to_translate, sr_translation))\n",
    "\n",
    "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a162dd",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "Tensor of sentences to translate has shape: (64, 14)\n",
    "Tensor of right-shifted translations has shape: (64, 15)\n",
    "Tensor of logits has shape: (64, 15, 12000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37009022",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_translator(Translator, Encoder, Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bc228",
   "metadata": {},
   "source": [
    "<a tên=\"3\"></a>\n",
    "## 3. Đào tạo\n",
    "\n",
    "Bây giờ bạn đã có một phiên bản chưa được huấn luyện của mô hình NMT, đã đến lúc huấn luyện nó. Bạn có thể sử dụng hàm `compile_and_train` bên dưới để đạt được điều này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a61ef65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87d9bf9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 48s 68ms/step - loss: 5.1730 - masked_acc: 0.2171 - masked_loss: 5.1756 - val_loss: 4.4001 - val_masked_acc: 0.3205 - val_masked_loss: 4.4010\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 17s 33ms/step - loss: 3.8393 - masked_acc: 0.4030 - masked_loss: 3.8401 - val_loss: 3.1249 - val_masked_acc: 0.4911 - val_masked_loss: 3.1270\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 2.8200 - masked_acc: 0.5347 - masked_loss: 2.8215 - val_loss: 2.4602 - val_masked_acc: 0.5754 - val_masked_loss: 2.4607\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 2.2937 - masked_acc: 0.6065 - masked_loss: 2.2946 - val_loss: 2.0259 - val_masked_acc: 0.6419 - val_masked_loss: 2.0265\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 1.9214 - masked_acc: 0.6588 - masked_loss: 1.9226 - val_loss: 1.7629 - val_masked_acc: 0.6764 - val_masked_loss: 1.7635\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.6584 - masked_acc: 0.6930 - masked_loss: 1.6594 - val_loss: 1.6269 - val_masked_acc: 0.6995 - val_masked_loss: 1.6264\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 1.5392 - masked_acc: 0.7090 - masked_loss: 1.5399 - val_loss: 1.5103 - val_masked_acc: 0.7097 - val_masked_loss: 1.5106\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 1.4257 - masked_acc: 0.7255 - masked_loss: 1.4266 - val_loss: 1.4045 - val_masked_acc: 0.7221 - val_masked_loss: 1.4062\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 1.3416 - masked_acc: 0.7355 - masked_loss: 1.3430 - val_loss: 1.3243 - val_masked_acc: 0.7325 - val_masked_loss: 1.3246\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 1.2103 - masked_acc: 0.7517 - masked_loss: 1.2117 - val_loss: 1.2477 - val_masked_acc: 0.7489 - val_masked_loss: 1.2487\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 1.1026 - masked_acc: 0.7649 - masked_loss: 1.1035 - val_loss: 1.1865 - val_masked_acc: 0.7589 - val_masked_loss: 1.1870\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.0810 - masked_acc: 0.7686 - masked_loss: 1.0819 - val_loss: 1.1546 - val_masked_acc: 0.7567 - val_masked_loss: 1.1548\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 1.0542 - masked_acc: 0.7710 - masked_loss: 1.0554 - val_loss: 1.1465 - val_masked_acc: 0.7574 - val_masked_loss: 1.1472\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.0277 - masked_acc: 0.7749 - masked_loss: 1.0288 - val_loss: 1.0936 - val_masked_acc: 0.7704 - val_masked_loss: 1.0942\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.9172 - masked_acc: 0.7898 - masked_loss: 0.9183 - val_loss: 1.0800 - val_masked_acc: 0.7679 - val_masked_loss: 1.0813\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.8711 - masked_acc: 0.7964 - masked_loss: 0.8718 - val_loss: 1.0797 - val_masked_acc: 0.7724 - val_masked_loss: 1.0814\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 0.8766 - masked_acc: 0.7951 - masked_loss: 0.8773 - val_loss: 1.0084 - val_masked_acc: 0.7783 - val_masked_loss: 1.0098\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.8797 - masked_acc: 0.7950 - masked_loss: 0.8806 - val_loss: 1.0087 - val_masked_acc: 0.7809 - val_masked_loss: 1.0096\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 0.8569 - masked_acc: 0.7984 - masked_loss: 0.8576 - val_loss: 0.9916 - val_masked_acc: 0.7822 - val_masked_loss: 0.9914\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.7484 - masked_acc: 0.8161 - masked_loss: 0.7493 - val_loss: 0.9852 - val_masked_acc: 0.7845 - val_masked_loss: 0.9860\n"
     ]
    }
   ],
   "source": [
    "# Train the translator (this takes some minutes so feel free to take a break)\n",
    "\n",
    "trained_translator, history = compile_and_train(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b9301",
   "metadata": {},
   "source": [
    "<a tên=\"4\"></a>\n",
    "## 4. Sử dụng mô hình để suy luận\n",
    "\n",
    "\n",
    "Bây giờ mô hình của bạn đã được đào tạo, bạn có thể sử dụng nó để suy luận. Để giúp bạn thực hiện việc này, chức năng `generate_next_token` được cung cấp. Lưu ý rằng hàm này được sử dụng bên trong vòng lặp for, do đó bạn cung cấp cho nó thông tin của bước trước để tạo thông tin cho bước tiếp theo. Đặc biệt, bạn cần theo dõi trạng thái của LSTM chú ý trước trong bộ giải mã và liệu bạn đã hoàn thành việc dịch hay chưa. Cũng lưu ý rằng biến `nhiệt độ` được đưa ra để xác định cách chọn mã thông báo tiếp theo dựa trên nhật ký dự đoán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "522f6b6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    \"\"\"Generates the next token in the sequence\n",
    "\n",
    "    Args:\n",
    "        decoder (Decoder): The decoder\n",
    "        context (tf.Tensor): Encoded sentence to translate\n",
    "        next_token (tf.Tensor): The predicted next token\n",
    "        done (bool): True if the translation is complete\n",
    "        state (list[tf.Tensor, tf.Tensor]): Hidden states of the pre-attention LSTM layer\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(tf.Tensor, np.float, list[tf.Tensor, tf.Tensor], bool): The next token, log prob of said token, hidden state of LSTM and if translation is done\n",
    "    \"\"\"\n",
    "    # Get the logits and state from the decoder\n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "    \n",
    "    # Trim the intermediate dimension \n",
    "    logits = logits[:, -1, :]\n",
    "        \n",
    "    # If temp is 0 then next_token is the argmax of logits\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "        \n",
    "    # If temp is not 0 then next_token is sampled out of logits\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "    \n",
    "    # Trim dimensions of size 1\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    \n",
    "    # Get the logit of the selected next_token\n",
    "    logit = logits[next_token].numpy()\n",
    "    \n",
    "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
    "    next_token = tf.reshape(next_token, shape=(1,1))\n",
    "    \n",
    "    # If next_token is End-of-Sentence token you are done\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "    \n",
    "    return next_token, logit, state, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d2d76",
   "metadata": {},
   "source": [
    "Xem cách nó hoạt động bằng cách chạy ô sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9937547a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [[10026]]\n",
      "Logit: -18.8343\n",
      "Done? False\n"
     ]
    }
   ],
   "source": [
    "# PROCESS SENTENCE TO TRANSLATE AND ENCODE\n",
    "\n",
    "# A sentence you wish to translate\n",
    "eng_sentence = \"I love languages\"\n",
    "\n",
    "# Convert it to a tensor\n",
    "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
    "\n",
    "# Vectorize it and pass it through the encoder\n",
    "context = english_vectorizer(texts).to_tensor()\n",
    "context = encoder(context)\n",
    "\n",
    "# SET STATE OF THE DECODER\n",
    "\n",
    "# Next token is Start-of-Sentence since you are starting fresh\n",
    "next_token = tf.fill((1,1), sos_id)\n",
    "\n",
    "# Hidden and Cell states of the LSTM can be mocked using uniform samples\n",
    "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
    "\n",
    "# You are not done until next token is EOS token\n",
    "done = False\n",
    "\n",
    "# Generate next token\n",
    "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
    "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170323dd",
   "metadata": {},
   "source": [
    "<a name=\"ex5\"></a>\n",
    "## Bài tập 5 - dịch\n",
    "\n",
    "Bây giờ bạn có thể kết hợp mọi thứ lại với nhau để dịch một câu nhất định. Để làm điều này, hãy hoàn thành chức năng `translate` bên dưới. Chức năng này sẽ đảm nhiệm các bước sau:\n",
    "- Xử lý câu để dịch và mã hóa nó\n",
    "\n",
    "+ Đặt trạng thái ban đầu của bộ giải mã\n",
    "\n",
    "- Nhận dự đoán về mã thông báo tiếp theo (bắt đầu bằng mã thông báo \\<SOS>) cho số lần lặp tối đa (trong trường hợp mã thông báo \\<EOS> không bao giờ được trả lại)\n",
    "\n",
    "+ Trả về văn bản đã dịch (dưới dạng chuỗi), logit của lần lặp cuối cùng (điều này giúp đo lường mức độ chắc chắn rằng toàn bộ chuỗi đã được dịch) và bản dịch ở định dạng mã thông báo.\n",
    "\n",
    "\n",
    "Gợi ý:\n",
    "\n",
    "- Ô trước cung cấp nhiều thông tin chi tiết về cách hoạt động của chức năng này, vì vậy nếu bạn gặp khó khăn, hãy tham khảo ô đó.\n",
    "\n",
    "+ Một số tài liệu hữu ích:\n",
    "+ [tf.newaxis](https://www.tensorflow.org/api_docs/python/tf#newaxis)\n",
    "\n",
    "- [tf.fill](https://www.tensorflow.org/api_docs/python/tf/fill)\n",
    "\n",
    "+ [tf.zeros](https://www.tensorflow.org/api_docs/python/tf/zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42c74f1f",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: translate\n",
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    \"\"\"Translate a given sentence from English to Portuguese\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained translator\n",
    "        text (string): The sentence to translate\n",
    "        max_length (int, optional): The maximum length of the translation. Defaults to 50.\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted <EOS> token and the tokenized translation\n",
    "    \"\"\"\n",
    "    # Lists to save tokens and logits\n",
    "    tokens, logits = [], []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # PROCESS THE SENTENCE TO TRANSLATE\n",
    "    \n",
    "    # Convert the original string into a tensor\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    \n",
    "    # Vectorize the text using the correct vectorizer\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    \n",
    "    # Get the encoded context (pass the context through the encoder)\n",
    "    # Hint: Remember you can get the encoder by using model.encoder\n",
    "    context = model.encoder(context)\n",
    "    \n",
    "    # INITIAL STATE OF THE DECODER\n",
    "    \n",
    "    # First token should be SOS token with shape (1,1)\n",
    "    next_token = tf.fill((1, 1), sos_id)\n",
    "    \n",
    "    # Initial hidden and cell states should be tensors of zeros with shape (1, UNITS)\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    \n",
    "    # You are done when you draw a EOS token as next token (initial state is False)\n",
    "    done = False\n",
    "\n",
    "    # Iterate for max_length iterations\n",
    "    for _ in range(max_length):\n",
    "        # Generate the next token\n",
    "        try:\n",
    "            next_token, logit, state, done = generate_next_token(\n",
    "                decoder=model.decoder,\n",
    "                context=context,\n",
    "                next_token=next_token,\n",
    "                done=done,\n",
    "                state=state,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        except:\n",
    "             raise Exception(\"Problem generating the next token\")\n",
    "        \n",
    "        # If done then break out of the loop\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # Add next_token to the list of tokens\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        # Add logit to the list of logits\n",
    "        logits.append(logit)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Concatenate all tokens into a tensor\n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "    \n",
    "    # Convert the translated tokens into text\n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "    \n",
    "    return translation, logits[-1], tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525e8ba",
   "metadata": {},
   "source": [
    "Hãy thử hàm của bạn với nhiệt độ bằng 0, hàm này sẽ mang lại đầu ra xác định và tương đương với giải mã tham lam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "daaea8c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu amo idiomas voce tem chovido .\n",
      "Translation tokens:[[   9  522  850   14   33 4211    4]]\n",
      "Logit: -0.403\n"
     ]
    }
   ],
   "source": [
    "# Running this cell multiple times should return the same output since temp is 0\n",
    "\n",
    "temp = 0.0 \n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05129b",
   "metadata": {},
   "source": [
    "Hãy thử chức năng của bạn với nhiệt độ 0,7 (đầu ra ngẫu nhiên):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e0697db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu eu amo idiomas ainda estao famosos .\n",
      "Translation tokens:[[   9    9  522  850   61   71 4140    4]]\n",
      "Logit: -0.353\n"
     ]
    }
   ],
   "source": [
    "# Running this cell multiple times should return different outputs since temp is not 0\n",
    "# You can try different temperatures\n",
    "\n",
    "temp = 0.7\n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3a9ea35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_translate(translate, trained_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba027524",
   "metadata": {},
   "source": [
    "<a tên=\"5\"></a>\n",
    "## 5. Giải mã rủi ro Bayes tối thiểu\n",
    "\n",
    "Như đã đề cập trong các bài giảng, việc nhận được mã thông báo có khả năng xảy ra cao nhất ở mỗi bước có thể không nhất thiết mang lại kết quả tốt nhất. Một cách tiếp cận khác là thực hiện Giải mã rủi ro Bayes tối thiểu hoặc MBR. Các bước chung để thực hiện điều này là:\n",
    "\n",
    "- Lấy một số mẫu ngẫu nhiên\n",
    "+ Cho điểm từng mẫu so với tất cả các mẫu khác\n",
    "- Chọn người có số điểm cao nhất\n",
    "\n",
    "Bạn sẽ xây dựng các hàm trợ giúp cho các bước này trong các phần sau.\n",
    "\n",
    "Với khả năng tạo ra các bản dịch khác nhau bằng cách đặt các giá trị nhiệt độ khác nhau, bạn có thể thực hiện những gì bạn đã thấy trong bài giảng và tạo ra một loạt bản dịch, sau đó xác định bản dịch nào là phù hợp nhất. Bây giờ bạn sẽ thực hiện việc này bằng cách sử dụng hàm `generate_samples` được cung cấp. Hàm này sẽ trả về bất kỳ số lượng bản dịch ứng cử viên mong muốn nào cùng với xác suất ghi nhật ký cho mỗi bản dịch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62301cd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
    "    \n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    # Iterate for n_samples iterations\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        # Save the logit and the translated tensor\n",
    "        _, logp, sample = translate(model, text, temperature=temperature)\n",
    "        \n",
    "        # Save the translated tensors\n",
    "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
    "        \n",
    "        # Save the logits\n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06bd792c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated tensor: [9, 522, 850, 14, 33, 1232, 8115, 4] has logit: -1.212\n",
      "Translated tensor: [9, 522, 850, 14, 33, 156, 304, 4] has logit: -0.798\n",
      "Translated tensor: [9, 255, 850, 12, 256, 10] has logit: -0.375\n",
      "Translated tensor: [9, 811, 850, 14, 33, 149, 4] has logit: -0.441\n"
     ]
    }
   ],
   "source": [
    "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
    "\n",
    "for s, l in zip(samples, log_probs):\n",
    "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b10677",
   "metadata": {},
   "source": [
    "## So sánh sự trùng lặp\n",
    "\n",
    "Bây giờ bạn có thể tạo nhiều bản dịch, đã đến lúc nghĩ ra một phương pháp để đo lường mức độ tốt của từng bản dịch. Như bạn đã thấy trong các bài giảng, một cách để đạt được điều này là so sánh từng mẫu với các mẫu khác.\n",
    "\n",
    "Có một số số liệu bạn có thể sử dụng cho mục đích này, như được trình bày trong các bài giảng và bạn có thể thử thử nghiệm bất kỳ số liệu nào trong số này. Đối với bài tập này, bạn sẽ tính điểm cho **sự trùng lặp unigram**.\n",
    "\n",
    "Một trong những số liệu này là [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) được sử dụng rộng rãi nhưng đơn giản, có giao điểm trên sự kết hợp của hai bộ. Hàm `jaccard_similarity` trả về số liệu này cho bất kỳ cặp bản dịch ứng viên và bản dịch tham chiếu nào:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edb54a71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "        \n",
    "    # Convert the lists to sets to get the unique tokens\n",
    "    candidate_set = set(candidate)\n",
    "    reference_set = set(reference)\n",
    "    \n",
    "    # Get the set of tokens common to both candidate and reference\n",
    "    common_tokens = candidate_set.intersection(reference_set)\n",
    "    \n",
    "    # Get the set of all tokens found in either candidate or reference\n",
    "    all_tokens = candidate_set.union(reference_set)\n",
    "    \n",
    "    # Compute the percentage of overlap (divide the number of common tokens by the number of all tokens)\n",
    "    overlap = len(common_tokens) / len(all_tokens)\n",
    "        \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc3384bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "\n",
    "js = jaccard_similarity(l1, l2)\n",
    "\n",
    "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6997662",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "jaccard similarity between tensors: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2510e3d",
   "metadata": {},
   "source": [
    "<a name=\"ex6\"></a>\n",
    "## Bài tập 6 - rouge1_similarity\n",
    "\n",
    "Độ tương tự của Jaccard là tốt nhưng thước đo được sử dụng phổ biến hơn trong dịch máy là điểm ROUGE. Đối với unigram, cái này được gọi là ROUGE-1 và như được trình bày trong bài giảng, bạn có thể xuất điểm cho cả độ chính xác và khả năng thu hồi khi so sánh hai mẫu. Để có được điểm số cuối cùng, bạn sẽ muốn tính điểm F1 như sau:\n",
    "\n",
    "$$score = 2* \\frac{(độ chính xác * thu hồi)}{(độ chính xác + thu hồi)}$$\n",
    "\n",
    "Để triển khai hàm `rouge1_similarity`, bạn muốn sử dụng lớp [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) từ thư viện chuẩn Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb2e0a00",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rouge1_similarity\n",
    "def rouge1_similarity(candidate, reference):\n",
    "    \"\"\"Computes the ROUGE 1 score between two token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list[int]): Tokenized candidate translation\n",
    "        reference (list[int]): Tokenized reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: Overlap between the two token lists\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Make a frequency table of the candidate and reference tokens\n",
    "    # Hint: use the Counter class (already imported)\n",
    "    candidate_word_counts = Counter(candidate)\n",
    "    reference_word_counts = Counter(reference)\n",
    "    \n",
    "    # Initialize overlap at 0\n",
    "    overlap = 0\n",
    "    \n",
    "    # Iterate over the tokens in the candidate frequency table\n",
    "    # Hint: Counter is a subclass of dict and you can get the keys \n",
    "    #       out of a dict using the keys method like this: dict.keys()\n",
    "    for token in candidate_word_counts.keys():\n",
    "        \n",
    "        # Get the count of the current token in the candidate frequency table\n",
    "        # Hint: You can access the counts of a token as you would access values of a dictionary\n",
    "        token_count_candidate = candidate_word_counts[token]\n",
    "        \n",
    "        # Get the count of the current token in the reference frequency table\n",
    "        # Hint: You can access the counts of a token as you would access values of a dictionary\n",
    "        token_count_reference = reference_word_counts[token]\n",
    "        \n",
    "        # Update the overlap by getting the minimum between the two token counts above\n",
    "        overlap += min(token_count_candidate, token_count_reference)\n",
    "    \n",
    "    # Compute the precision\n",
    "    # Hint: precision = overlap / (number of tokens in candidate list) \n",
    "    precision = overlap / len(candidate)\n",
    "    \n",
    "    # Compute the recall\n",
    "    # Hint: recall = overlap / (number of tokens in reference list) \n",
    "    recall = overlap / len(reference)\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "        # Compute the Rouge1 Score\n",
    "        # Hint: This is equivalent to the F1 score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        return f1_score\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return 0 # If precision + recall = 0 then return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14bb5295",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "\n",
    "r1s = rouge1_similarity(l1, l2)\n",
    "\n",
    "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8c61a",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a680132e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_rouge1_similarity(rouge1_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8a058",
   "metadata": {},
   "source": [
    "## Tính tổng điểm\n",
    "\n",
    "\n",
    "Bây giờ bạn sẽ xây dựng một hàm để tạo ra điểm tổng thể cho một mẫu cụ thể. Như đã đề cập trong bài giảng, bạn cần so sánh từng mẫu với tất cả các mẫu khác. Ví dụ: nếu tạo ra 30 câu, chúng ta sẽ cần so sánh câu 1 với câu 2 đến câu 30. Sau đó, chúng ta so sánh câu 2 với câu 1 và 3 đến câu 30, v.v. Ở mỗi bước, chúng tôi lấy điểm trung bình của tất cả các so sánh để có được điểm tổng thể cho một mẫu cụ thể. Để minh họa, đây sẽ là các bước để tạo điểm của danh sách 4 mẫu.\n",
    "\n",
    "- Lấy điểm tương đồng giữa mẫu 1 và mẫu 2\n",
    "+ Lấy điểm tương đồng giữa mẫu 1 và mẫu 3\n",
    "- Lấy điểm tương đồng giữa mẫu 1 và mẫu 4\n",
    "+ Lấy điểm trung bình của 3 bước đầu tiên. Đây sẽ là tổng điểm của mẫu 1\n",
    "- Lặp lại cho đến khi mẫu từ 1 đến 4 có tổng điểm.\n",
    "\n",
    "\n",
    "Kết quả sẽ được lưu vào từ điển để dễ dàng tra cứu.\n",
    "\n",
    "<a name=\"ex7\"></a>\n",
    "## Bài tập 7 - Average_overlap\n",
    "\n",
    "Hoàn thành hàm `average_overlap` bên dưới để triển khai quy trình được mô tả ở trên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "142264ff",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: average_overlap\n",
    "def average_overlap(samples, similarity_fn):\n",
    "    \"\"\"Computes the arithmetic mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list[list[int]]): Tokenized version of translated sentences\n",
    "        similarity_fn (Function): Similarity function used to compute the overlap\n",
    "\n",
    "    Returns:\n",
    "        dict[int, float]: A dictionary mapping the index of each translation to its score\n",
    "    \"\"\"\n",
    "    # Initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # Iterate through all samples (enumerate helps keep track of indexes)\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "                \n",
    "        # Initially overlap is zero\n",
    "        overlap = 0\n",
    "        \n",
    "        # Iterate through all samples (enumerate helps keep track of indexes)\n",
    "        for index_sample, sample in enumerate(samples):\n",
    "\n",
    "            # Skip if the candidate index is the same as the sample index\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # Get the overlap between candidate and sample using the similarity function\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            # Add the sample overlap to the total overlap\n",
    "            overlap += sample_overlap\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Get the score for the candidate by computing the average\n",
    "        score = overlap / (len(samples) - 1)\n",
    "\n",
    "        # Only use 3 decimal points\n",
    "        score = round(score, 3)\n",
    "        \n",
    "        # Save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f36cf403",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
      "\n",
      "{0: 0.45, 1: 0.625, 2: 0.575}\n"
     ]
    }
   ],
   "source": [
    "# Test with Jaccard similarity\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "\n",
    "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
    "\n",
    "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277aed2-a5c9-4ed0-9ee2-614939f2df7b",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
    "\n",
    "{0: 0.45, 1: 0.625, 2: 0.575}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d961a304-7c03-4ecb-ba5f-c8747ed3ec39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n",
      "\n",
      "{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n"
     ]
    }
   ],
   "source": [
    "# Test with Rouge1 similarity\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "l4 = [5,6]\n",
    "\n",
    "avg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n",
    "\n",
    "print(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30adc749-ffcb-4e82-a8f0-c04a7e39da0a",
   "metadata": {},
   "source": [
    "##### __Kết quả mong đợi__\n",
    "\n",
    "```\n",
    "average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n",
    "\n",
    "{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c41b1fba-fd0f-41e6-9b07-746f64030fe3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_average_overlap(average_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4482249",
   "metadata": {},
   "source": [
    "Trong thực tế, người ta cũng thường thấy giá trị trung bình có trọng số được sử dụng để tính điểm tổng thể thay vì chỉ giá trị trung bình số học. Điều này được triển khai trong hàm `weighted_avg_overlap` bên dưới và bạn có thể sử dụng nó trong các thử nghiệm của mình để xem cái nào sẽ cho kết quả tốt hơn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "398714be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
    "    \n",
    "    # Scores dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # Iterate over the samples\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        # Initialize overlap and weighted sum\n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        \n",
    "        # Iterate over all samples and log probabilities\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            # Skip if the candidate index is the same as the sample index            \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # Convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            # Update the weighted sum\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            # Get the unigram overlap between candidate and sample\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            # Update the overlap\n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        # Compute the score for the candidate\n",
    "        score = overlap / weight_sum\n",
    "\n",
    "        # Only use 3 decimal points\n",
    "        score = round(score, 3)\n",
    "        \n",
    "        # Save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3dfd6d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted average overlap using Jaccard similarity is:\n",
      "\n",
      "{0: 0.443, 1: 0.631, 2: 0.558}\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "log_probs = [0.4, 0.2, 0.5]\n",
    "\n",
    "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
    "\n",
    "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0b4db",
   "metadata": {},
   "source": [
    "## mbr_decode\n",
    "\n",
    "Bây giờ bạn sẽ tập hợp mọi thứ lại với nhau trong hàm `mbr_decode` bên dưới. Bước cuối cùng này không được chấm điểm vì hàm này chỉ là phần bao bọc xung quanh tất cả nội dung thú vị mà bạn đã mã hóa cho đến nay!\n",
    "\n",
    "Bạn có thể sử dụng nó để thử nghiệm, thử số lượng mẫu, nhiệt độ và hàm tương tự khác nhau!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6fcfa640",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
    "    \n",
    "    # Generate samples\n",
    "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
    "    \n",
    "    # Compute the overlap scores\n",
    "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
    "\n",
    "    # Decode samples\n",
    "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
    "    \n",
    "    # Find the key with the highest score\n",
    "    max_score_key = max(scores, key=lambda k: scores[k])\n",
    "    \n",
    "    # Get the translation \n",
    "    translation = decoded_translations[max_score_key]\n",
    "    \n",
    "    return translation, decoded_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99507fcc-7727-45e7-933b-d3a08034f731",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation candidates:\n",
      "eu amo idiomas voce tem alguma estante de tempo . ! eu eu planejo .\n",
      "eu amo idiomas voce tem que ?\n",
      "eu eu amo idiomas voce e de pao .\n",
      "eu amor idiomas a gente ?\n",
      "eu adoro idiomas ?\n",
      "eu amo idiomas voces de vitela .\n",
      "eu amo idiomas voce tem comida .\n",
      "eu adorei idiomas ?\n",
      "eu adoro idiomas voce .\n",
      "eu eu ama idiomas voce tem certeza .\n",
      "\n",
      "Selected translation: eu amo idiomas voce tem comida .\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"I love languages\"\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b193f-4ea6-4ca1-ae29-a506cce656d9",
   "metadata": {},
   "source": [
    "**Xin chúc mừng!** Tuần tới, bạn sẽ tìm hiểu sâu hơn về các mô hình chú ý và nghiên cứu kiến ​​trúc Máy biến áp. Bạn sẽ xây dựng một mạng khác nhưng không có phần lặp lại. Nó sẽ cho thấy rằng sự chú ý là tất cả những gì bạn cần! Thật thú vị!\n",
    "\n",
    "**Hãy tiếp tục phát huy!**"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
