{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f368f78e",
   "metadata": {
    "colab_type": "text",
    "id": "7yuytuIllsv1"
   },
   "source": [
    "# Bài 3: Trả lời câu hỏi\n",
    "\n",
    "Chào mừng bạn đến với bài tập thứ ba của khóa học 4. Trong bài tập này, bạn sẽ khám phá cách trả lời câu hỏi. Bạn sẽ triển khai \"Chuyển văn bản sang văn bản từ Transformers\" (hay còn gọi là T5). Vì bạn đã triển khai máy biến áp từ đầu vào tuần trước nên giờ đây bạn có thể sử dụng chúng.\n",
    "\n",
    "<img src = \"images/qa.png\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ec561",
   "metadata": {
    "colab_type": "text",
    "id": "Db6LQW5cMSgx"
   },
   "source": [
    "## Mục lục\n",
    "\n",
    "- [Overview](#0-1)\n",
    "- [Importing the Packages](#0-2)\n",
    "- [1 - Prepare the data for pretraining T5](#1)\n",
    "- [1.1 - Pre-Training Objective](#1-1)\n",
    "- [1.2 - C4 Dataset](#1-2)\n",
    "- [1.3 - Process C4](#1-3)\n",
    "- [1.4 - Decode to Natural Language](#1-4)\n",
    "- [1.5 - Tokenizing and Masking](#1-5)\n",
    "- [Exercise 1 - tokenize_and_mask](#ex-1)\n",
    "- [1.6 - Creating the Pairs](#1-6)\n",
    "- [2 - Pretrain a T5 model using C4](#2)\n",
    "- [2.1 - Instantiate a new transformer model](#2-1)\n",
    "- [2.2 - C4 pretraining](#2-2)\n",
    "- [3 - Fine tune the T5 model for Question Answering](#3)\n",
    "- [3.1 - Creating a list of paired question and answers](#3-1)\n",
    "- [Exercise 2 - Parse the SQuaD 2.0 dataset](#ex-2)\n",
    "- [3.2 - Fine tune the T5 model](#3-2)\n",
    "- [3.3 - Implement your Question Answering model](#3-3)\n",
    "- [Exercise 3 - Implement the question answering function](#ex-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595e9c4",
   "metadata": {
    "colab_type": "text",
    "id": "ysxogfC1M158"
   },
   "source": [
    "<a name='0-1'></a>\n",
    "## Tổng quan\n",
    "\n",
    "Nhiệm vụ này sẽ khác với hai nhiệm vụ trước. Do hạn chế về bộ nhớ của môi trường này và vì lý do thời gian, mô hình của bạn sẽ được đào tạo với các tập dữ liệu nhỏ, vì vậy bạn sẽ không nhận được các mô hình mà bạn có thể sử dụng trong sản xuất nhưng bạn sẽ có được kiến ​​thức cần thiết về cách hoạt động của các mô hình Ngôn ngữ Sáng tạo được đào tạo và sử dụng. Ngoài ra, bạn sẽ không dành quá nhiều thời gian cho kiến ​​trúc của các mô hình mà thay vào đó, bạn sẽ sử dụng một mô hình đã được đào tạo trước trên tập dữ liệu lớn hơn và tinh chỉnh nó để có kết quả tốt hơn.\n",
    "\n",
    "Sau khi hoàn thành bài thí nghiệm này, bạn sẽ:\n",
    "* Hiểu cách cấu trúc tập dữ liệu C4.\n",
    "* Huấn luyện trước mô hình máy biến áp bằng Mô hình ngôn ngữ đeo mặt nạ.\n",
    "* Hiểu cách hoạt động của mô hình \"Chuyển văn bản sang văn bản từ Transformers\" hoặc T5.\n",
    "* Tinh chỉnh mô hình T5 để trả lời câu hỏi\n",
    "\n",
    "Trước khi bắt đầu, hãy dành chút thời gian để đọc những lời khuyên sau:\n",
    "#### MẸO ĐỂ ĐÁNH GIÁ THÀNH CÔNG BÀI VIẾT CỦA BẠN:\n",
    "- Tất cả các ô đều bị đóng băng ngoại trừ những ô bạn cần gửi giải pháp của mình.\n",
    "- Bạn có thể thêm các ô mới để thử nghiệm nhưng những ô này sẽ bị người chấm bỏ qua, vì vậy, đừng dựa vào các ô mới tạo để lưu trữ mã giải pháp của bạn, hãy sử dụng các vị trí được cung cấp cho việc này.\n",
    "- Bạn có thể thêm nhận xét #grade-up-to-here vào bất kỳ ô đã chấm điểm nào để báo hiệu cho học sinh chấm điểm rằng học sinh chỉ được đánh giá đến điểm đó. Điều này rất hữu ích nếu bạn muốn kiểm tra xem mình có đang đi đúng hướng hay không ngay cả khi bạn chưa hoàn thành toàn bộ nhiệm vụ. Hãy nhớ xóa bình luận sau đó!\n",
    "- Để gửi sổ ghi chép của bạn, hãy lưu nó rồi nhấp vào nút gửi màu xanh lam ở đầu trang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156cf78",
   "metadata": {},
   "source": [
    "<a name='0-2'></a>\n",
    "## Nhập gói\n",
    "\n",
    "Hãy bắt đầu bằng cách nhập tất cả các thư viện cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a532381",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "uDhi6qLQMHzs",
    "outputId": "64947d91-eef3-425b-9b4b-7ca7cefcc823",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "import json\n",
    "from termcolor import colored\n",
    "import string\n",
    "import textwrap\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow_text as tf_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import transformer_utils \n",
    "import utils\n",
    "\n",
    "# Will come in handy later\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf711eba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import w3_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bea693",
   "metadata": {
    "colab_type": "text",
    "id": "t7A-LAxsYpDd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='1'></a>\n",
    "##1 - Chuẩn bị data cho pretraining T5\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Mục tiêu trước đào tạo\n",
    "\n",
    "Trong giai đoạn đầu đào tạo mô hình T5 cho nhiệm vụ Trả lời câu hỏi, quy trình đào tạo trước bao gồm việc tận dụng mô hình ngôn ngữ đeo mặt nạ (MLM) trên một tập dữ liệu rất lớn, chẳng hạn như tập dữ liệu C4. Mục tiêu là cho phép mô hình học cách biểu diễn các từ và cụm từ theo ngữ cảnh, thúc đẩy sự hiểu biết sâu sắc hơn về ngữ nghĩa ngôn ngữ. Để bắt đầu đào tạo trước, điều cần thiết là phải sử dụng kiến ​​trúc Transformer, kiến ​​trúc này tạo thành xương sống của T5. Cơ chế tự chú ý của Transformer cho phép mô hình cân nhắc các phần khác nhau của chuỗi đầu vào một cách linh hoạt, nắm bắt các phần phụ thuộc tầm xa một cách hiệu quả.\n",
    "\n",
    "Trước khi đi sâu vào đào tạo trước, việc xử lý trước dữ liệu kỹ lưỡng là rất quan trọng. Bộ dữ liệu C4, một bộ sưu tập các trang web đa dạng và phong phú, cung cấp nguồn phong phú cho các nhiệm vụ hiểu ngôn ngữ. Tập dữ liệu cần được mã hóa thành các đơn vị nhỏ hơn, chẳng hạn như từ phụ hoặc từ, để tạo điều kiện thuận lợi cho việc nhập mô hình. Ngoài ra, văn bản thường được phân đoạn thành các chuỗi hoặc lô có độ dài cố định, tối ưu hóa hiệu quả tính toán trong quá trình đào tạo.\n",
    "\n",
    "Đối với mục tiêu lập mô hình ngôn ngữ được che giấu, một tỷ lệ phần trăm đầu vào được mã hóa được che giấu ngẫu nhiên và mô hình được đào tạo để dự đoán nội dung gốc của các mã thông báo được che giấu này. Quá trình này khuyến khích mô hình T5 nắm bắt các mối quan hệ theo ngữ cảnh giữa các từ và cụm từ, nâng cao khả năng tạo ra các phản hồi mạch lạc và phù hợp với ngữ cảnh trong các nhiệm vụ tiếp theo như trả lời câu hỏi.\n",
    "\n",
    "Tóm lại, quá trình đào tạo trước mô hình T5 bao gồm việc sử dụng kiến ​​trúc Transformer trên tập dữ liệu lớn như C4, kết hợp với quá trình xử lý trước dữ liệu tỉ mỉ để chuyển đổi văn bản thô thành định dạng phù hợp cho việc đào tạo. Việc kết hợp mục tiêu mô hình hóa ngôn ngữ mặt nạ đảm bảo rằng mô hình học các cách biểu diễn theo ngữ cảnh mạnh mẽ, tạo nền tảng vững chắc cho việc tinh chỉnh tiếp theo đối với các tác vụ cụ thể như trả lời câu hỏi.\n",
    "\n",
    "**Lưu ý:** Từ \"mặt nạ\" sẽ được sử dụng trong suốt bài tập này trong bối cảnh ẩn/xóa (các) từ\n",
    "\n",
    "Bạn sẽ triển khai Mô hình ngôn ngữ đeo mặt nạ (MLM) như trong hình ảnh sau.\n",
    "\n",
    "<img src = \"images/loss.png\" width=\"600\" height = \"400\">\n",
    "\n",
    "Giả sử bạn có văn bản sau: <span style=\"color:blue\"> **Cảm ơn bạn <span style=\"color:red\">vì đã mời </span> tôi đến bữa tiệc của bạn <span style=\"color:red\" >tuần trước</span>** </span>\n",
    "\n",
    "\n",
    "Bây giờ, khi nhập dữ liệu, bạn sẽ che các từ màu đỏ trong văn bản:\n",
    "\n",
    "<span style=\"color:blue\"> **Đầu vào:**</span> Cảm ơn bạn **X** tôi đã đến dự bữa tiệc **Y** của bạn trong tuần.\n",
    "\n",
    "<span style=\"color:blue\"\">**Đầu ra:**</span> Mô hình sẽ dự đoán (các) từ cho **X** và **Y**.\n",
    "\n",
    "**[EOS]** sẽ được sử dụng để đánh dấu sự kết thúc của chuỗi mục tiêu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc25302",
   "metadata": {
    "colab_type": "text",
    "id": "Cwr7LoXwQUW5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Bộ dữ liệu C4\n",
    "\n",
    "[C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), còn được gọi là Common Crawl C4 (Common Crawl Corpus C4), là tập dữ liệu quy mô lớn về các trang web được thu thập bởi [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4). Nó thường được sử dụng cho các nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau và nghiên cứu học máy. Mỗi mẫu trong tập dữ liệu C4 tuân theo một định dạng nhất quán, giúp mẫu này phù hợp với các mô hình đào tạo trước như BERT. Dưới đây là phần giải thích và mô tả ngắn gọn về tập dữ liệu C4:\n",
    "\n",
    "- Định dạng: Mỗi mẫu trong tập dữ liệu C4 được biểu diễn dưới dạng một đối tượng JSON, chứa một số cặp khóa-giá trị.\n",
    "\n",
    "- Nội dung: Trường “text” trong mỗi mẫu chứa nội dung văn bản thực tế được trích xuất từ ​​các trang web. Văn bản này thường bao gồm nhiều chủ đề và phong cách viết khác nhau nên đa dạng và phù hợp với các mô hình ngôn ngữ đào tạo.\n",
    "\n",
    "- Siêu dữ liệu: Tập dữ liệu bao gồm siêu dữ liệu như 'độ dài nội dung', 'loại nội dung', 'dấu thời gian' và 'url', cung cấp thông tin bổ sung về từng trang web. 'Độ dài nội dung' chỉ định độ dài của nội dung, 'loại nội dung' mô tả loại nội dung (ví dụ: 'văn bản/thuần túy'), 'dấu thời gian' cho biết thời điểm trang web được thu thập thông tin và 'url' cung cấp nguồn URL của trang web.\n",
    "\n",
    "- Ứng dụng: Bộ dữ liệu C4 được sử dụng phổ biến để huấn luyện và tinh chỉnh các mô hình ngôn ngữ quy mô lớn như BERT. Nó phục vụ như một nguồn tài nguyên có giá trị cho các nhiệm vụ như phân loại văn bản, nhận dạng thực thể được đặt tên, trả lời câu hỏi, v.v.\n",
    "\n",
    "- Kích thước: Bộ dữ liệu C4 chứa hơn 800 GiB dữ liệu văn bản, phù hợp cho các mô hình đào tạo với hàng tỷ tham số.\n",
    "\n",
    "Chạy ô bên dưới để xem tập dữ liệu C4 trông như thế nào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa56acc9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example number 1: \n",
      "\n",
      "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'} \n",
      "\n",
      "example number 2: \n",
      "\n",
      "{'text': 'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.'} \n",
      "\n",
      "example number 3: \n",
      "\n",
      "{'text': 'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.'} \n",
      "\n",
      "example number 4: \n",
      "\n",
      "{'text': \"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\"} \n",
      "\n",
      "example number 5: \n",
      "\n",
      "{'text': 'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what’s included in the mill levy measure.'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load example jsons\n",
    "with open('data/c4-en-10k.jsonl', 'r') as file:\n",
    "    example_jsons = [json.loads(line.strip()) for line in file]\n",
    "\n",
    "# Printing the examples to see how the data looks like\n",
    "for i in range(5):\n",
    "    print(f'example number {i+1}: \\n\\n{example_jsons[i]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48901d97",
   "metadata": {
    "colab_type": "text",
    "id": "eeihIgtiaSfh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Quy trình C4\n",
    "\n",
    "Với mục đích lấy trước mẫu T5 các bạn sẽ chỉ sử dụng `nội dung` của mỗi mục. Trong đoạn mã sau, bạn chỉ lọc trường `văn bản` từ tất cả các mục trong tập dữ liệu. Đây là dữ liệu bạn sẽ sử dụng để tạo `đầu vào` và `đích` cho mô hình ngôn ngữ của mình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af728cb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
      "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n"
     ]
    }
   ],
   "source": [
    "# Grab text field from dictionary\n",
    "natural_language_texts = [example_json['text'] for example_json in example_jsons]\n",
    "\n",
    "# Print the first text example\n",
    "print(natural_language_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a25a2",
   "metadata": {
    "colab_type": "text",
    "id": "1rMrONRqcCYi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - Giải mã sang ngôn ngữ tự nhiên\n",
    "\n",
    "[SentencePieceTokenizer](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer), được sử dụng trong đoạn mã, mã hóa văn bản thành các đơn vị từ phụ, nâng cao khả năng xử lý các cấu trúc từ phức tạp, các từ không có từ vựng và hỗ trợ đa ngôn ngữ. Nó đơn giản hóa quá trình tiền xử lý, đảm bảo mã thông báo nhất quán và tích hợp liền mạch với các khung máy học.\n",
    "\n",
    "Trong tác vụ này, một mô hình SentencePiece được tải từ một tệp, được sử dụng để mã hóa văn bản thành các từ phụ được biểu thị bằng ID số nguyên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac53d57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "# PAD, EOS = 0, 1\n",
    "\n",
    "with open(\"./models/sentencepiece.model\", \"rb\") as f:\n",
    "    pre_trained_tokenizer = f.read()\n",
    "    \n",
    "tokenizer = tf_text.SentencepieceTokenizer(pre_trained_tokenizer, out_type=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b0e86",
   "metadata": {},
   "source": [
    "Trong mã thông báo này, chuỗi `</s>` được sử dụng làm mã thông báo `EOS`. Theo mặc định, tokenizer không thêm `EOS` vào cuối mỗi câu, vì vậy bạn cần thêm thủ công khi được yêu cầu. Hãy xác minh id nào tương ứng với mã thông báo này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2fec4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: 1\n"
     ]
    }
   ],
   "source": [
    "eos = tokenizer.string_to_id(\"</s>\").numpy()\n",
    "\n",
    "print(\"EOS: \" + str(eos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87756f",
   "metadata": {},
   "source": [
    "Mã này hiển thị quá trình mã hóa các từ riêng lẻ từ một văn bản nhất định, trong trường hợp này là mục nhập đầu tiên của tập dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c48352",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "deletable": false,
    "id": "iCCjgiVZgTSK",
    "outputId": "023a227c-d895-4fd9-ae83-9394fe48cebd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t-->\tTokenization\n",
      "----------------------------------------\n",
      "Foil    \t-->\t[4452, 173]\n",
      "plaid   \t-->\t[30772]\n",
      "lycra   \t-->\t[3, 120, 2935]\n",
      "and     \t-->\t[11]\n",
      "spandex \t-->\t[8438, 26, 994]\n",
      "shortall\t-->\t[710, 1748]\n",
      "with    \t-->\t[28]\n",
      "metallic\t-->\t[18813]\n",
      "slinky  \t-->\t[3, 7, 4907, 63]\n",
      "insets. \t-->\t[16, 2244, 7, 5]\n",
      "Attached\t-->\t[28416, 15, 26]\n",
      "metallic\t-->\t[18813]\n",
      "elastic \t-->\t[15855]\n",
      "belt    \t-->\t[6782]\n",
      "with    \t-->\t[28]\n",
      "O-ring. \t-->\t[411, 18, 1007, 5]\n",
      "Headband\t-->\t[3642, 3348]\n",
      "included.\t-->\t[1285, 5]\n",
      "Great   \t-->\t[1651]\n",
      "hip     \t-->\t[5436]\n",
      "hop     \t-->\t[13652]\n",
      "or      \t-->\t[42]\n",
      "jazz    \t-->\t[9948]\n",
      "dance   \t-->\t[2595]\n",
      "costume.\t-->\t[11594, 5]\n",
      "Made    \t-->\t[6465]\n",
      "in      \t-->\t[16]\n",
      "the     \t-->\t[8]\n",
      "USA.    \t-->\t[2312, 5]\n"
     ]
    }
   ],
   "source": [
    "# printing the encoding of each word to see how subwords are tokenized\n",
    "tokenized_text = [(list(tokenizer.tokenize(word).numpy()), word) for word in natural_language_texts[2].split()]\n",
    "\n",
    "print(\"Word\\t\\t-->\\tTokenization\")\n",
    "print(\"-\"*40)\n",
    "for element in tokenized_text:\n",
    "    print(f\"{element[1]:<8}\\t-->\\t{element[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4616cf3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Và như thường lệ, thư viện cung cấp chức năng biến mã thông báo số thành văn bản mà con người có thể đọc được. Hãy nhìn cách nó hoạt động."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d7037b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: [12847   277]\n",
      "detokenized: b'Beginners'\n"
     ]
    }
   ],
   "source": [
    "# We can see that detokenize successfully undoes the tokenization\n",
    "print(f\"tokenized: {tokenizer.tokenize('Beginners')}\\ndetokenized: {tokenizer.detokenize(tokenizer.tokenize('Beginners'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f63624",
   "metadata": {
    "colab_type": "text",
    "id": "vPKgGOeOxv3w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Như bạn có thể thấy ở trên, bạn có thể lấy một đoạn chuỗi và mã hóa nó.\n",
    "\n",
    "Bây giờ bạn sẽ tạo các cặp `input` và `target` cho phép bạn huấn luyện mô hình của mình. T5 sử dụng các id ở cuối tệp vocab làm trọng điểm. Ví dụ: nó sẽ thay thế:\n",
    "- `vocab_size - 1` bởi `<Z>`\n",
    "- `vocab_size - 2` bởi `<Y>`\n",
    "- và kể từ đó trở đi.\n",
    "\n",
    "Nó gán cho mỗi từ một `chr`.\n",
    "\n",
    "Hàm `pretty_decode` bên dưới mà bạn sẽ sử dụng sau đây sẽ giúp xử lý loại khi giải mã. Hãy xem và cố gắng hiểu chức năng này đang làm gì.\n",
    "\n",
    "\n",
    "Thông báo rằng:\n",
    "```python\n",
    "string.ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "```\n",
    "\n",
    "**LƯU Ý:** Các mục tiêu có thể có nhiều hơn 52 lính canh mà chúng tôi thay thế, nhưng đây chỉ là để giúp bạn hình dung về mọi thứ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25bb46d",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "fCPQL5FTxv3w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sentinels(tokenizer, display=False):\n",
    "    sentinels = {}\n",
    "    vocab_size = tokenizer.vocab_size(name=None)\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = tokenizer.detokenize([vocab_size - i]).numpy().decode(\"utf-8\")\n",
    "        \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'    \n",
    "    \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "\n",
    "    return sentinels\n",
    "\n",
    "def pretty_decode(encoded_str_list, sentinels, tokenizer):\n",
    "    # If already a string, just do the replacements.\n",
    "    if tf.is_tensor(encoded_str_list) and encoded_str_list.dtype == tf.string:\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = tf.strings.regex_replace(encoded_str_list, token, char)\n",
    "        return encoded_str_list\n",
    "  \n",
    "    # We need to decode and then prettyfy it.\n",
    "    return pretty_decode(tokenizer.detokenize(encoded_str_list), sentinels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d75b6c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "fCPQL5FTxv3w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentinel is <Z> and the decoded token is: Internațional\n",
      "The sentinel is <Y> and the decoded token is: erwachsene\n",
      "The sentinel is <X> and the decoded token is: Cushion\n",
      "The sentinel is <W> and the decoded token is: imunitar\n",
      "The sentinel is <V> and the decoded token is: Intellectual\n",
      "The sentinel is <U> and the decoded token is: traditi\n",
      "The sentinel is <T> and the decoded token is: disguise\n",
      "The sentinel is <S> and the decoded token is: exerce\n",
      "The sentinel is <R> and the decoded token is: nourishe\n",
      "The sentinel is <Q> and the decoded token is: predominant\n",
      "The sentinel is <P> and the decoded token is: amitié\n",
      "The sentinel is <O> and the decoded token is: erkennt\n",
      "The sentinel is <N> and the decoded token is: dimension\n",
      "The sentinel is <M> and the decoded token is: inférieur\n",
      "The sentinel is <L> and the decoded token is: refugi\n",
      "The sentinel is <K> and the decoded token is: cheddar\n",
      "The sentinel is <J> and the decoded token is: unterlieg\n",
      "The sentinel is <I> and the decoded token is: garanteaz\n",
      "The sentinel is <H> and the decoded token is: făcute\n",
      "The sentinel is <G> and the decoded token is: réglage\n",
      "The sentinel is <F> and the decoded token is: pedepse\n",
      "The sentinel is <E> and the decoded token is: Germain\n",
      "The sentinel is <D> and the decoded token is: distinctly\n",
      "The sentinel is <C> and the decoded token is: Schraub\n",
      "The sentinel is <B> and the decoded token is: emanat\n",
      "The sentinel is <A> and the decoded token is: trimestre\n",
      "The sentinel is <z> and the decoded token is: disrespect\n",
      "The sentinel is <y> and the decoded token is: Erasmus\n",
      "The sentinel is <x> and the decoded token is: Australia\n",
      "The sentinel is <w> and the decoded token is: permeabil\n",
      "The sentinel is <v> and the decoded token is: deseori\n",
      "The sentinel is <u> and the decoded token is: manipulated\n",
      "The sentinel is <t> and the decoded token is: suggér\n",
      "The sentinel is <s> and the decoded token is: corespund\n",
      "The sentinel is <r> and the decoded token is: nitro\n",
      "The sentinel is <q> and the decoded token is: oyons\n",
      "The sentinel is <p> and the decoded token is: Account\n",
      "The sentinel is <o> and the decoded token is: échéan\n",
      "The sentinel is <n> and the decoded token is: laundering\n",
      "The sentinel is <m> and the decoded token is: genealogy\n",
      "The sentinel is <l> and the decoded token is: QuickBooks\n",
      "The sentinel is <k> and the decoded token is: constituted\n",
      "The sentinel is <j> and the decoded token is: Fertigung\n",
      "The sentinel is <i> and the decoded token is: goutte\n",
      "The sentinel is <h> and the decoded token is: regulă\n",
      "The sentinel is <g> and the decoded token is: overwhelmingly\n",
      "The sentinel is <f> and the decoded token is: émerg\n",
      "The sentinel is <e> and the decoded token is: broyeur\n",
      "The sentinel is <d> and the decoded token is: povești\n",
      "The sentinel is <c> and the decoded token is: emulator\n",
      "The sentinel is <b> and the decoded token is: halloween\n",
      "The sentinel is <a> and the decoded token is: combustibil\n"
     ]
    }
   ],
   "source": [
    "sentinels = get_sentinels(tokenizer, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73a35d",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPQL5FTxv3w",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Bây giờ, hãy sử dụng hàm `pretty_decode` trong câu sau. Lưu ý rằng tất cả các từ được liệt kê là trọng điểm, sẽ được thay thế bằng hàm có trọng điểm tương ứng. Đó có thể là một nhược điểm của phương pháp này, nhưng bây giờ đừng lo lắng về nó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe92253",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'I want to dress up as an <V> this <b>.'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_decode(tf.constant(\"I want to dress up as an Intellectual this halloween.\"), sentinels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b04b7",
   "metadata": {
    "colab_type": "text",
    "id": "Y64F--Nzxv30",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Các hàm trên làm cho `đầu vào` và `đích` của bạn dễ đọc hơn. Ví dụ: bạn có thể thấy nội dung như thế này khi triển khai chức năng tạo mặt nạ bên dưới.\n",
    "\n",
    "- <span style=\"color:red\"> Câu đầu vào: </span> Younes và Lukasz đã cùng làm việc trong phòng thí nghiệm sau bữa trưa ngày hôm qua.\n",
    "- <span style=\"color:red\">Đầu vào: </span> Younes và Lukasz **Z** cùng nhau ở **Y** ngày hôm qua sau bữa trưa.\n",
    "- <span style=\"color:red\">Mục tiêu: </span> **Z** đang làm việc tại phòng thí nghiệm **Y**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244cd7a8",
   "metadata": {
    "colab_type": "text",
    "id": "NvvNd7n6xv30"
   },
   "source": [
    "<a name='1-5'></a>\n",
    "### 1.5 - Token hóa và che giấu\n",
    "\n",
    "Trong nhiệm vụ này, bạn sẽ triển khai hàm `tokenize_and_mask`, hàm này mã hóa và che dấu các từ đầu vào dựa trên một xác suất nhất định. Xác suất được kiểm soát bởi tham số `noise`, thường được đặt để che khoảng `15%` số từ trong văn bản đầu vào. Hàm sẽ tạo hai danh sách các chuỗi được mã hóa theo thuật toán được nêu bên dưới:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050f25c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='ex-1'></a>\n",
    "### Bài tập 1 - tokenize_and_mask\n",
    "\n",
    "- Bắt đầu với hai danh sách trống: `inps` và `targs`\n",
    "- Mã hóa văn bản đầu vào bằng cách sử dụng mã thông báo đã cho.\n",
    "- Đối với mỗi `token` trong chuỗi token hóa:\n",
    "- Tạo số ngẫu nhiên (mô phỏng việc tung đồng xu có trọng số)\n",
    "- Nếu giá trị ngẫu nhiên lớn hơn ngưỡng cho trước(nhiễu):\n",
    "- Thêm token hiện tại vào danh sách `inps`\n",
    "- Khác:\n",
    "- Nếu phải thêm lính canh mới (đọc ghi chú **):\n",
    "- Tính toán ID trọng điểm tiếp theo bằng cách sử dụng tiến trình.\n",
    "- Thêm canh gác vào `inps` và `targs` để đánh dấu vị trí của phần tử bị che.\n",
    "- Thêm token hiện tại vào danh sách `targs`.\n",
    "\n",
    "** Có một trường hợp đặc biệt cần xem xét. Nếu hai hoặc nhiều mã thông báo liên tiếp bị che trong quá trình này, bạn không cần thêm trọng điểm mới vào chuỗi. Để giải quyết vấn đề này, hãy sử dụng cờ `prev_no_mask`, bắt đầu là `True` nhưng được chuyển thành `False` mỗi khi bạn ẩn một phần tử mới. Mã thêm điểm canh gác sẽ chỉ được thực thi nếu trước khi ẩn mã thông báo, cờ ở trạng thái `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c660bf97",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Bi33WKgRxv31",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: tokenize_and_mask\n",
    "def tokenize_and_mask(text, \n",
    "                      noise=0.15, \n",
    "                      randomizer=np.random.uniform, \n",
    "                      tokenizer=None):\n",
    "    \"\"\"Tokenizes and masks a given input.\n",
    "\n",
    "    Args:\n",
    "        text (str or bytes): Text input.\n",
    "        noise (float, optional): Probability of masking a token. Defaults to 0.15.\n",
    "        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.\n",
    "        tokenizer (function, optional): Tokenizer function. Defaults to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        inps, targs: Lists of integers associated to inputs and targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Current sentinel number (starts at 0)\n",
    "    cur_sentinel_num = 0\n",
    "    \n",
    "    # Inputs and targets\n",
    "    inps, targs = [], []\n",
    "\n",
    "    # Vocab_size\n",
    "    vocab_size = int(tokenizer.vocab_size())\n",
    "    \n",
    "    # EOS token id \n",
    "    # Must be at the end of each target!\n",
    "    eos = tokenizer.string_to_id(\"</s>\").numpy()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # prev_no_mask is True if the previous token was NOT masked, False otherwise\n",
    "    # set prev_no_mask to True\n",
    "    prev_no_mask = True\n",
    "    \n",
    "    # Loop over the tokenized text\n",
    "    for token in tokenizer.tokenize(text).numpy():\n",
    "        \n",
    "        # Generate a random value between 0 and 1\n",
    "        rnd_val = randomizer() \n",
    "        \n",
    "        # Check if the noise is greater than a random value (weighted coin flip)\n",
    "        if noise > rnd_val:\n",
    "            \n",
    "            # Check if previous token was NOT masked\n",
    "            if prev_no_mask:\n",
    "                \n",
    "                # Current sentinel increases by 1\n",
    "                cur_sentinel_num += 1\n",
    "                \n",
    "                # Compute end_id by subtracting current sentinel value out of the total vocabulary size\n",
    "                end_id = vocab_size - cur_sentinel_num\n",
    "                \n",
    "                # Append end_id at the end of the targets\n",
    "                targs.append(end_id)\n",
    "                \n",
    "                # Append end_id at the end of the inputs\n",
    "                inps.append(end_id)\n",
    "                \n",
    "            # Append token at the end of the targets\n",
    "            targs.append(token)\n",
    "            \n",
    "            # set prev_no_mask accordingly\n",
    "            prev_no_mask = False\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Append token at the end of the inputs\n",
    "            inps.append(token)\n",
    "            \n",
    "            # Set prev_no_mask accordingly\n",
    "            prev_no_mask = True\n",
    "    \n",
    "    \n",
    "    # Add EOS token to the end of the targets\n",
    "    targs.append(eos)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inps, targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92edca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "OlPySQo9xv34",
    "outputId": "2b0dc5e4-8d58-4eb0-a146-0c9f158264ac",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized inputs - shape=53:\n",
      "\n",
      "[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5]\n",
      "\n",
      "targets - shape=19:\n",
      "\n",
      "[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 1]\n"
     ]
    }
   ],
   "source": [
    "# Some logic to mock a np.random value generator\n",
    "# Needs to be in the same cell for it to always generate same output\n",
    "def testing_rnd():\n",
    "    def dummy_generator():\n",
    "        vals = np.linspace(0, 1, 10)\n",
    "        cyclic_vals = itertools.cycle(vals)\n",
    "        for _ in range(100):\n",
    "            yield next(cyclic_vals)\n",
    "\n",
    "    dumr = itertools.cycle(dummy_generator())\n",
    "\n",
    "    def dummy_randomizer():\n",
    "        return next(dumr)\n",
    "    \n",
    "    return dummy_randomizer\n",
    "\n",
    "input_str = 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers.'\n",
    "\n",
    "inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd(), tokenizer=tokenizer)\n",
    "print(f\"tokenized inputs - shape={len(inps)}:\\n\\n{inps}\\n\\ntargets - shape={len(targs)}:\\n\\n{targs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07996252",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### **Đầu ra dự kiến:**\n",
    "```\n",
    "tokenized inputs - shape=53:\n",
    "\n",
    "[31999 15068  4501     3 12297  3399    16  5964  7115 31998   531    25\n",
    "   241    12   129   394    44   492 31997    58   148    56    43     8\n",
    "  1004     6   474 31996    39  4793   230     5  2721     6  1600  1630\n",
    " 31995  1150  4501 15068 16127     6  9137  2659  5595 31994   782  3624\n",
    " 14627    15 12612   277     5]\n",
    "\n",
    "targets - shape=19:\n",
    "\n",
    "[31999 12847   277 31998     9    55 31997  3326 15068 31996    48    30\n",
    " 31995   727  1715 31994    45   301     1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76daaa5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "w3_unittest.test_tokenize_and_mask(tokenize_and_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87bea8",
   "metadata": {
    "colab_type": "text",
    "id": "_omCqbkLxv36",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Bây giờ bạn sẽ sử dụng thông tin đầu vào và mục tiêu từ hàm `tokenize_and_mask` mà bạn đã triển khai ở trên. Hãy xem phiên bản được giải mã của câu bị che bằng cách sử dụng `inps` và `targs` từ câu trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "054d51bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "y6xwo6lGxv37",
    "outputId": "4330ae1e-1805-40c9-daf3-c6bbe92d957b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      " b'<Z> BBQ Class Taking Place in Missoul <Y> Do you want to get better at making <X>? You will have the opportunity, put <W> your calendar now. Thursday, September 22 <V> World Class BBQ Champion, Tony Balay <U>onestar Smoke Rangers.'\n",
      "\n",
      "Targets: \n",
      "\n",
      " b'<Z> Beginners <Y>a! <X> delicious BBQ <W> this on <V>nd join <U> from L'\n"
     ]
    }
   ],
   "source": [
    "print('Inputs: \\n\\n', pretty_decode(inps, sentinels, tokenizer).numpy())\n",
    "print('\\nTargets: \\n\\n', pretty_decode(targs, sentinels, tokenizer).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707c320",
   "metadata": {
    "colab_type": "text",
    "id": "24HZiIBLxv3-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='1-6'></a>\n",
    "### 1.6 - Tạo cặp\n",
    "\n",
    "Bây giờ bạn sẽ tạo các cặp bằng cách sử dụng tập dữ liệu của mình. Bạn sẽ lặp lại dữ liệu của mình và tạo các cặp (inp, targ) bằng cách sử dụng các hàm mà chúng tôi đã cung cấp cho bạn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae83fff0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply tokenize_and_mask\n",
    "inputs_targets_pairs = [tokenize_and_mask(text.encode('utf-8', errors='ignore').decode('utf-8'), tokenizer=tokenizer) \n",
    "                        for text in natural_language_texts[0:2000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f157ad1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "c1HiKreWokhs",
    "outputId": "fc194524-41de-4d3b-87d9-ae35c29c9f79",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "\n",
      "inputs:\n",
      "<Z>il plaid <Y>lycra <X> spandex shortall with metallic slinky\n",
      "<W>sets. Attache <V> metallic elastic belt with O <U>ring. Head <T>\n",
      "included. Great hip hop<S> jazz dance costume.<R> in the USA.\n",
      "\n",
      "targets:\n",
      "<Z> Fo <Y>  <X> and <W> in <V>d <U>- <T>band<S> or<R> Made\n",
      "\n",
      "\n",
      "\n",
      "[2]\n",
      "\n",
      "inputs:\n",
      "I thought I was going to <Z> 3rd season <Y> Wire tonight. <X> there\n",
      "was a commentary <W> 11, so I had to re <V>watch <U> Ground with <T>\n",
      "commentary. Hopefully<S> can finish<R> season <Q>.\n",
      "\n",
      "targets:\n",
      "<Z> finish the <Y> of the <X> But <W> on episode <V>- <U> Middle <T>\n",
      "the<S> I<R> the <Q> next weekend\n",
      "\n",
      "\n",
      "\n",
      "[3]\n",
      "\n",
      "inputs:\n",
      "Pencarian <Z>FILM Untuk \" <Y>eace <X>er 2017 <W> yuk mampir ke channel\n",
      "say <V>. Edges <U> provides the l.. A corrupt cop makes one w.. <T>er\n",
      "2017  ⁇ <S> ⁇  .. Náo Lo ⁇ n - Peace Break.. Please subscribe and hit\n",
      "..<R> in HD at http://.. <Q> cannot believe I manage..\n",
      "\n",
      "targets:\n",
      "<Z>  <Y>P <X> Break <W>\" <V>. <U> East <T> Peace Break<S> <R> uploaded\n",
      "<Q> I\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_input_target_pairs(inputs_targets_pairs, sentinels, wrapper=textwrap.TextWrapper(width=70), tokenizer=tokenizer):\n",
    "    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\n",
    "        inps, tgts = inp_tgt_pair\n",
    "        inps = str(pretty_decode(inps, sentinels, tokenizer).numpy(), encoding='utf-8')\n",
    "        tgts = str(pretty_decode(tgts, sentinels, tokenizer).numpy(), encoding='utf-8')\n",
    "        print(f'[{i}]\\n\\n'\n",
    "              f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n",
    "              f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n')\n",
    "\n",
    "# Print 3 samples. We print inputs with less than 100 tokens. It is just to give you and idea of the process\n",
    "display_input_target_pairs(filter(lambda x: len(x[0]) < 100, inputs_targets_pairs[0:12]), sentinels, wrapper, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5e6d9",
   "metadata": {
    "colab_type": "text",
    "id": "hQI5Jgov5X-d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='2'></a>\n",
    "##2 - Pretrain mẫu T5 sử dụng C4\n",
    "\n",
    "Bây giờ bạn sẽ sử dụng kiến ​​trúc của Transformer mà bạn đã mã hóa trong bài tập trước để tóm tắt văn bản, nhưng lần này là để trả lời các câu hỏi. Thay vì đào tạo mô hình trả lời câu hỏi từ đầu, trước tiên bạn sẽ “huấn luyện trước” mô hình bằng tập dữ liệu C4 vừa xử lý. Điều này sẽ giúp mô hình tìm hiểu cấu trúc chung của ngôn ngữ từ một tập dữ liệu lớn. Điều này dễ thực hiện hơn nhiều vì bạn không cần gắn nhãn cho bất kỳ dữ liệu nào mà chỉ cần sử dụng mặt nạ, việc này được thực hiện tự động. Sau đó, bạn sẽ sử dụng dữ liệu từ bộ SQuAD để hướng dẫn mô hình trả lời các câu hỏi theo ngữ cảnh. Để bắt đầu, chúng ta hãy xem lại kiến ​​trúc của Transformer.\n",
    "\n",
    "<img src = \"images/fulltransformer.png\" width=\"300\" height=\"600\">\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Khởi tạo mô hình máy biến áp mới\n",
    "\n",
    "Chúng tôi đã đóng gói mã được triển khai vào tuần trước vào tệp `Transformer.py`. Bạn có thể nhập nó vào đây và thiết lập với cùng cấu hình được sử dụng ở đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58ce75dc",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "eScMhEG7xv4H",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "fully_connected_dim = 128\n",
    "num_heads = 2\n",
    "positional_encoding_length = 256\n",
    "\n",
    "encoder_vocab_size = int(tokenizer.vocab_size())\n",
    "decoder_vocab_size = encoder_vocab_size\n",
    "\n",
    "# Initialize the model\n",
    "transformer = transformer_utils.Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads, \n",
    "    fully_connected_dim,\n",
    "    encoder_vocab_size, \n",
    "    decoder_vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618697cf",
   "metadata": {},
   "source": [
    "Bây giờ, bạn sẽ xác định trình tối ưu hóa và hàm mất mát. Đối với nhiệm vụ này, mô hình sẽ cố gắng dự đoán các từ bị che, do đó, giống như trong bài thực hành trước, hàm mất sẽ là `SparseCategoricalCrossEntropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7df2d1d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = transformer_utils.CustomSchedule(embedding_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# Here you will store the losses, so you can later plot them\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d54376",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Cch đào tạo lại\n",
    "\n",
    "Để đào tạo mô hình Tensorflow, bạn cần sắp xếp dữ liệu thành các tập dữ liệu. Bây giờ, bạn sẽ nhận được `đầu vào` và `mục tiêu` cho mô hình máy biến áp từ `inputs_targets_pairs`. Trước khi tạo tập dữ liệu, bạn cần đảm bảo rằng tất cả `đầu vào` đều có cùng độ dài bằng cách cắt bớt các chuỗi dài hơn và đệm các chuỗi ngắn hơn bằng `0`. Điều tương tự phải được thực hiện cho các mục tiêu. Hàm `tf.keras.preprocessing.sequence.pad_sequences` sẽ giúp bạn ở đây, như trong bài tập tuần trước.\n",
    "\n",
    "Bạn sẽ sử dụng `BATCH_SIZE = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b03eb998",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit the size of the input and output data so this can run in this environment\n",
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences([x[0] for x in inputs_targets_pairs], maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences([x[1] for x in inputs_targets_pairs], maxlen=decoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)\n",
    "\n",
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32ae0c",
   "metadata": {},
   "source": [
    "Bây giờ, bạn có thể chạy vòng huấn luyện trong 10 kỷ nguyên. Việc chạy nó với tập dữ liệu lớn như C4 trên một máy tính tốt có đủ bộ nhớ và GPU tốt có thể mất hơn 24 giờ. Tại đây, bạn sẽ chạy một số kỷ nguyên bằng cách sử dụng một phần nhỏ của tập dữ liệu C4 để minh họa. Sẽ chỉ mất vài phút nhưng mô hình sẽ không mạnh mẽ lắm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc5f76",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    number_of_batches=len(list(enumerate(dataset)))\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
    "        transformer_utils.train_step(inp, tar, transformer, loss_object, optimizer, train_loss)\n",
    "    \n",
    "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
    "    losses.append(train_loss.result())\n",
    "    \n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "\n",
    "# Save the pretrained model\n",
    "# transformer.save_weights('./model_c4_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8135b5",
   "metadata": {},
   "source": [
    "**Tải mô hình đã được huấn luyện trước**\n",
    "\n",
    "Để cho thấy mô hình này thực sự mạnh mẽ đến mức nào, chúng tôi đã đào tạo mô hình này trong nhiều kỷ nguyên với tập dữ liệu đầy đủ trong Colab và lưu trọng số cho bạn. Bạn có thể tải chúng bằng cách sử dụng ô bên dưới. Đối với phần còn lại của cuốn sổ, bạn sẽ thấy được sức mạnh của việc học chuyển giao trong thực tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55360633",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f9be075d9a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_weights('./pretrained_models/model_c4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8822756",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Tinh chỉnh mẫu T5 để trả lời câu hỏi\n",
    "\n",
    "Bây giờ, bạn sẽ tinh chỉnh mô hình được huấn luyện trước để Trả lời câu hỏi bằng cách sử dụng [SQUad 2.0 dataset](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "SQuAD, viết tắt của Bộ dữ liệu trả lời câu hỏi Stanford, là bộ dữ liệu được thiết kế để đào tạo và đánh giá các hệ thống trả lời câu hỏi. Nó bao gồm các câu hỏi thực tế do con người đặt ra trên một tập hợp các bài viết Wikipedia, trong đó câu trả lời cho mỗi câu hỏi là một đoạn văn bản cụ thể trong bài viết tương ứng.\n",
    "\n",
    "SQuAD 1.1, phiên bản trước của bộ dữ liệu SQuAD, chứa hơn 100.000 cặp câu hỏi-câu trả lời trên khoảng 500 bài viết.\n",
    "SQuAD 2.0, chứa thêm 50.000 câu hỏi không cần trả lời. Bộ câu hỏi bổ sung này có thể giúp đào tạo các mô hình để phát hiện những câu hỏi không thể trả lời.\n",
    "\n",
    "Hãy tải tập dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "987571df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 442\n"
     ]
    }
   ],
   "source": [
    "with open('data/train-v2.0.json', 'r') as f:\n",
    "    example_jsons = json.load(f)\n",
    "\n",
    "example_jsons = example_jsons['data']\n",
    "\n",
    "print('Number of articles: ' + str(len(example_jsons)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941761f",
   "metadata": {},
   "source": [
    "Cấu trúc của mỗi bài viết như sau:\n",
    "- `title`: Tiêu đề bài viết\n",
    "- `paragraphs`: Danh sách các đoạn văn và câu hỏi liên quan đến chúng\n",
    "- `ngữ cảnh`: Văn bản đoạn văn thực tế\n",
    "- `was`: Bộ câu hỏi liên quan đến đoạn văn\n",
    "- `câu hỏi`: Một câu hỏi\n",
    "- `is`: Mã định danh duy nhất của câu hỏi\n",
    "- `is_imposible`: Boolean, chỉ định câu hỏi có thể được trả lời hay không\n",
    "- `answers`: Tập hợp các câu trả lời có thể có cho câu hỏi\n",
    "- `text`: Câu trả lời\n",
    "- `answer_start`: Chỉ mục của ký tự bắt đầu câu chứa câu trả lời rõ ràng cho câu hỏi\n",
    "\n",
    "Hãy xem một bài viết bằng cách chạy ô tiếp theo. Lưu ý rằng `ngữ cảnh` thường là thành phần cuối cùng của mỗi đoạn văn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c4c4cfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Beyoncé\n",
      "{'qas': [{'question': 'When did Beyonce start becoming popular?', 'id': '56be85543aeaaa14008c9063', 'answers': [{'text': 'in the late 1990s', 'answer_start': 269}], 'is_impossible': False}, {'question': 'What areas did Beyonce compete in when she was growing up?', 'id': '56be85543aeaaa14008c9065', 'answers': [{'text': 'singing and dancing', 'answer_start': 207}], 'is_impossible': False}, {'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'id': '56be85543aeaaa14008c9066', 'answers': [{'text': '2003', 'answer_start': 526}], 'is_impossible': False}, {'question': 'In what city and state did Beyonce  grow up? ', 'id': '56bf6b0f3aeaaa14008c9601', 'answers': [{'text': 'Houston, Texas', 'answer_start': 166}], 'is_impossible': False}, {'question': 'In which decade did Beyonce become famous?', 'id': '56bf6b0f3aeaaa14008c9602', 'answers': [{'text': 'late 1990s', 'answer_start': 276}], 'is_impossible': False}, {'question': 'In what R&B group was she the lead singer?', 'id': '56bf6b0f3aeaaa14008c9603', 'answers': [{'text': \"Destiny's Child\", 'answer_start': 320}], 'is_impossible': False}, {'question': 'What album made her a worldwide known artist?', 'id': '56bf6b0f3aeaaa14008c9604', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}], 'is_impossible': False}, {'question': \"Who managed the Destiny's Child group?\", 'id': '56bf6b0f3aeaaa14008c9605', 'answers': [{'text': 'Mathew Knowles', 'answer_start': 360}], 'is_impossible': False}, {'question': 'When did Beyoncé rise to fame?', 'id': '56d43c5f2ccc5a1400d830a9', 'answers': [{'text': 'late 1990s', 'answer_start': 276}], 'is_impossible': False}, {'question': \"What role did Beyoncé have in Destiny's Child?\", 'id': '56d43c5f2ccc5a1400d830aa', 'answers': [{'text': 'lead singer', 'answer_start': 290}], 'is_impossible': False}, {'question': 'What was the first album Beyoncé released as a solo artist?', 'id': '56d43c5f2ccc5a1400d830ab', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}], 'is_impossible': False}, {'question': 'When did Beyoncé release Dangerously in Love?', 'id': '56d43c5f2ccc5a1400d830ac', 'answers': [{'text': '2003', 'answer_start': 526}], 'is_impossible': False}, {'question': 'How many Grammy awards did Beyoncé win for her first solo album?', 'id': '56d43c5f2ccc5a1400d830ad', 'answers': [{'text': 'five', 'answer_start': 590}], 'is_impossible': False}, {'question': \"What was Beyoncé's role in Destiny's Child?\", 'id': '56d43ce42ccc5a1400d830b4', 'answers': [{'text': 'lead singer', 'answer_start': 290}], 'is_impossible': False}, {'question': \"What was the name of Beyoncé's first solo album?\", 'id': '56d43ce42ccc5a1400d830b5', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}], 'is_impossible': False}], 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'}\n"
     ]
    }
   ],
   "source": [
    "example_article = example_jsons[0]\n",
    "example_article\n",
    "\n",
    "print(\"Title: \" + example_article[\"title\"])\n",
    "print(example_article[\"paragraphs\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982be57",
   "metadata": {},
   "source": [
    "Bài viết trước có thể khó điều hướng nên đây là một đoạn ví dụ được định dạng đẹp mắt:\n",
    "```python\n",
    "{\n",
    "  \"context\": \"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles 'Crazy in Love' and 'Baby Boy'\",\n",
    "  \"qas\": [\n",
    "    {\n",
    "      \"question\": \"When did Beyonce start becoming popular?\",\n",
    "      \"id\": \"56be85543aeaaa14008c9063\",\n",
    "      \"answers\": [\n",
    "        {\n",
    "          \"text\": \"in the late 1990s\",\n",
    "          \"answer_start\": 269\n",
    "        }\n",
    "      ],\n",
    "      \"is_impossible\": false\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"What areas did Beyonce compete in when she was growing up?\",\n",
    "      \"id\": \"56be85543aeaaa14008c9065\",\n",
    "      \"answers\": [\n",
    "        {\n",
    "          \"text\": \"singing and dancing\",\n",
    "          \"answer_start\": 207\n",
    "        }\n",
    "      ],\n",
    "      \"is_impossible\": false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3345571",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Tạo danh sách câu hỏi và câu trả lời ghép đôi\n",
    "\n",
    "Bạn được giao nhiệm vụ tạo các cặp đầu vào/đầu ra cho mô hình Trả lời câu hỏi (QA) bằng cách sử dụng bộ dữ liệu SQuAD 2.0. Mỗi cặp theo cấu trúc:\n",
    "\n",
    "- đầu vào: `câu hỏi: <Q> bối cảnh: <P>`\n",
    "- mục tiêu: `trả lời: <A>`\n",
    "\n",
    "Ở đây, `<Q>` thể hiện câu hỏi trong ngữ cảnh của đoạn văn đã cho `<P>`, và `<A>` là một câu trả lời khả dĩ.\n",
    "\n",
    "Trong cuốn sổ tay này, chúng ta sẽ tập trung vào một câu trả lời duy nhất cho mỗi câu hỏi. Tuy nhiên, điều cần lưu ý là tập dữ liệu chứa các câu hỏi có nhiều câu trả lời. Khi đào tạo một mô hình trong các tình huống thực tế, hãy cân nhắc việc đưa vào tất cả thông tin có sẵn.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Bài tập 2 - Phân tích bộ dữ liệu SQuaD 2.0\n",
    "\n",
    "Nhiệm vụ của bạn là triển khai hàm pars_squad, hàm này lặp lại tất cả các bài viết, đoạn văn và câu hỏi trong tập dữ liệu SQuAD. Trích xuất các cặp đầu vào và mục tiêu cho mô hình QA bằng mẫu mã được cung cấp.\n",
    "- Bắt đầu với hai danh sách trống: `inputs` và `target`.\n",
    "- Lặp lại tất cả các bài viết trong tập dữ liệu.\n",
    "- Đối với mỗi bài viết, lặp lại từng đoạn văn.\n",
    "- Trích xuất ngữ cảnh trong đoạn văn.\n",
    "- Lặp lại từng câu hỏi trong đoạn văn đã cho.\n",
    "- Kiểm tra xem câu hỏi có phải là không thể thực hiện được và có ít nhất một câu trả lời hay không.\n",
    "- Nếu đáp ứng điều kiện trên thì tạo chuỗi `question_context` như mô tả trong cấu trúc đầu vào.\n",
    "- Tạo chuỗi `answer` bằng cách sử dụng câu trả lời đầu tiên trong số các câu trả lời có sẵn.\n",
    "- Nối `question_context` vào danh sách `inputs`.\n",
    "- Nối `answer` vào danh sách `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5344f35",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: parse_squad\n",
    "def parse_squad(dataset):\n",
    "    \"\"\"Extract all the answers/questions pairs from the SQuAD dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): The imported JSON dataset\n",
    "\n",
    "    Returns:\n",
    "        inputs, targets: Two lists containing the inputs and the targets for the QA model\n",
    "    \"\"\"\n",
    "\n",
    "    inputs, targets = [], []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Loop over all the articles\n",
    "    for article in dataset:\n",
    "        \n",
    "        # Loop over each paragraph of each article\n",
    "        for paragraph in article['paragraphs']:\n",
    "            \n",
    "            # Extract context from the paragraph\n",
    "            context = paragraph['context']\n",
    "            \n",
    "            #Loop over each question of the given paragraph\n",
    "            for qa in paragraph['qas']:\n",
    "                \n",
    "                # If this question is not impossible and there is at least one answer\n",
    "                if len(qa['answers']) > 0 and not(qa['is_impossible']):\n",
    "                    \n",
    "                    # Create the question/context sequence\n",
    "                    question_context = 'question: ' + qa['question'] + ' context: ' + context\n",
    "                    \n",
    "                    # Create the answer sequence. Use the text field of the first answer\n",
    "                    answer = 'answer: ' + qa['answers'][0]['text']\n",
    "                    \n",
    "                    # Add the question_context to the inputs list\n",
    "                    inputs.append(question_context)\n",
    "                    \n",
    "                    # Add the answer to the targets list\n",
    "                    targets.append(answer)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6744c424",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question/answer pairs: 86821\n",
      "\n",
      "First Q/A pair:\n",
      "\n",
      "inputs: \u001b[34mquestion: When did Beyonce start becoming popular? context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\u001b[0m\n",
      "\n",
      "targets: \u001b[32manswer: in the late 1990s\u001b[0m\n",
      "\n",
      "Last Q/A pair:\n",
      "\n",
      "inputs: \u001b[34mquestion: What is KMC an initialism of? context: Kathmandu Metropolitan City (KMC), in order to promote international relations has established an International Relations Secretariat (IRC). KMC's first international relationship was established in 1975 with the city of Eugene, Oregon, United States. This activity has been further enhanced by establishing formal relationships with 8 other cities: Motsumoto City of Japan, Rochester of the USA, Yangon (formerly Rangoon) of Myanmar, Xi'an of the People's Republic of China, Minsk of Belarus, and Pyongyang of the Democratic Republic of Korea. KMC's constant endeavor is to enhance its interaction with SAARC countries, other International agencies and many other major cities of the world to achieve better urban management and developmental programs for Kathmandu.\u001b[0m\n",
      "\n",
      "targets: \u001b[32manswer: Kathmandu Metropolitan City\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "inputs, targets =  parse_squad(example_jsons)          \n",
    "print(\"Number of question/answer pairs: \" + str(len(inputs)))\n",
    "\n",
    "print('\\nFirst Q/A pair:\\n\\ninputs: ' + colored(inputs[0], 'blue'))\n",
    "print('\\ntargets: ' + colored(targets[0], 'green'))\n",
    "print('\\nLast Q/A pair:\\n\\ninputs: ' + colored(inputs[-1], 'blue'))\n",
    "print('\\ntargets: ' + colored(targets[-1], 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b164c2",
   "metadata": {},
   "source": [
    "#### **Đầu ra dự kiến:**\n",
    "```\n",
    "Number of question/answer pairs: 86821\n",
    "\n",
    "First Q/A pair:\n",
    "\n",
    "inputs: question: When did Beyonce start becoming popular? context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
    "\n",
    "targets: answer: in the late 1990s\n",
    "\n",
    "Last Q/A pair:\n",
    "\n",
    "inputs: question: What is KMC an initialism of? context: Kathmandu Metropolitan City (KMC), in order to promote international relations has established an International Relations Secretariat (IRC). KMC's first international relationship was established in 1975 with the city of Eugene, Oregon, United States. This activity has been further enhanced by establishing formal relationships with 8 other cities: Motsumoto City of Japan, Rochester of the USA, Yangon (formerly Rangoon) of Myanmar, Xi'an of the People's Republic of China, Minsk of Belarus, and Pyongyang of the Democratic Republic of Korea. KMC's constant endeavor is to enhance its interaction with SAARC countries, other International agencies and many other major cities of the world to achieve better urban management and developmental programs for Kathmandu.\n",
    "\n",
    "targets: answer: Kathmandu Metropolitan City\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f197bb69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "w3_unittest.test_parse_squad(parse_squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f69b24",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Bạn sẽ sử dụng 40000 mẫu để đào tạo và 5000 mẫu để kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "947354ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 40K pairs for training\n",
    "inputs_train = inputs[0:40000] \n",
    "targets_train = targets[0:40000]  \n",
    "\n",
    "# 5K pairs for testing\n",
    "inputs_test = inputs[40000:45000] \n",
    "targets_test =  targets[40000:45000] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21fd31",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Bây giờ, bạn có thể tạo tập dữ liệu hàng loạt gồm các chuỗi được đệm. Trước tiên, bạn sẽ mã hóa các đầu vào và mục tiêu. Sau đó, bằng cách sử dụng hàm `tf.keras.preprocessing.sequence.pad_sequences`, bạn sẽ đảm bảo rằng đầu vào và đầu ra có độ dài cần thiết. Hãy nhớ rằng các chuỗi dài hơn kích thước yêu cầu sẽ bị cắt bớt và các chuỗi ngắn hơn sẽ được đệm bằng `0`. Thiết lập này rất giống với thiết lập khác được sử dụng trong sổ ghi chép này và sổ ghi chép trước đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83393c74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit the size of the input and output data so this can run in this environment\n",
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "inputs_str = [tokenizer.tokenize(s) for s in inputs_train]\n",
    "targets_str = [tf.concat([tokenizer.tokenize(s), [1]], 0) for s in targets_train]\n",
    "\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs_str, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets_str, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)\n",
    "\n",
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c8a9",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 Tinh chỉnh mẫu T5\n",
    "\n",
    "Bây giờ, bạn sẽ huấn luyện mô hình trong 2 kỷ nguyên. Trong mô hình T5, tất cả trọng số đều được điều chỉnh trong quá trình tinh chỉnh. Như thường lệ, việc tinh chỉnh mô hình này để đạt được kết quả hiện đại sẽ cần nhiều thời gian và tài nguyên hơn mức hiện có trong môi trường này. Tuy nhiên, bạn có thể đào tạo mô hình cho nhiều kỷ nguyên hơn và có nhiều dữ liệu hơn bằng cách sử dụng GPU Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaba558",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 2\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    number_of_batches=len(list(enumerate(dataset)))\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
    "        transformer_utils.train_step(inp, tar, transformer, loss_object, optimizer, train_loss)\n",
    "    \n",
    "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
    "    losses.append(train_loss.result())\n",
    "    \n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "    #if epoch % 15 == 0:\n",
    "        #transformer.save_weights('./pretrained_models/model_qa_temp')\n",
    "# Save the final model\n",
    "#transformer.save_weights('./pretrained_models/model_qa_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8dc0c",
   "metadata": {},
   "source": [
    "Để có được một mô hình hoạt động bình thường, bạn cần đào tạo trong khoảng 100 kỷ nguyên. Vì vậy, chúng tôi đã đào tạo trước một mô hình cho bạn. Chỉ cần tải trọng số trong mô hình hiện tại và sử dụng nó để trả lời các câu hỏi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "144e769b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f9d65afda60>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore the weights\n",
    "transformer.load_weights('./pretrained_models/model_qa3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e09fd",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Triển khai mô hình Trả lời câu hỏi của bạn\n",
    "Ở bước cuối cùng này, bạn sẽ triển khai hàm answer_question, sử dụng mô hình máy biến áp được huấn luyện trước để trả lời câu hỏi.\n",
    "\n",
    "Để giúp bạn sử dụng chức năng `transformer_utils.next_word` được cung cấp. Hàm này nhận câu hỏi và phần mở đầu của câu trả lời (cả ở định dạng tensor) cùng với mô hình để dự đoán mã thông báo tiếp theo trong câu trả lời. Ô tiếp theo hiển thị cách sử dụng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92b40de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word is: 'blue'\n",
      "Answer so far: 'answer: blue'\n"
     ]
    }
   ],
   "source": [
    "# Define an example question\n",
    "example_question = \"question: What color is the sky? context: Sky is blue\"\n",
    "\n",
    "# Question is tokenized and padded\n",
    "# Note that this is hardcoded here but you must implement this in the upcoming exercise\n",
    "tokenized_padded_question = tf.constant([[822, 10, 363, 945, 19, 8, 5796, 58, 2625, 10, 5643, 19, 1692, 0, 0]])\n",
    "\n",
    "# All answers begin with the string \"answer: \"\n",
    "# Feel free to check that this is indeed the tokenized version of that string\n",
    "tokenized_answer = tf.constant([[1525,   10]])\n",
    "\n",
    "# Predict the next word using the transformer_utils.next_word function\n",
    "# Notice that it expects the question, answer and model (in that order)\n",
    "next_word = transformer_utils.next_word(tokenized_padded_question, tokenized_answer, transformer)\n",
    "\n",
    "print(f\"Predicted next word is: '{tokenizer.detokenize(next_word).numpy()[0].decode('utf-8')}'\")\n",
    "\n",
    "# Concatenate predicted word with answer so far\n",
    "answer_so_far = tf.concat([tokenized_answer, next_word], axis=-1)\n",
    "\n",
    "print(f\"Answer so far: '{tokenizer.detokenize(answer_so_far).numpy()[0].decode('utf-8')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23a6be",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "### Bài tập 3 - Thực hiện chức năng trả lời câu hỏi\n",
    "\n",
    "Triển khai hàm `answer_question`. Dưới đây là các bước:\n",
    "- **Thiết lập câu hỏi:**\n",
    "\n",
    "- Mã hóa câu hỏi đã cho bằng cách sử dụng mã thông báo được cung cấp.\n",
    "- Thêm một chiều bổ sung cho tensor để tương thích.\n",
    "- Đệm tensor câu hỏi bằng cách sử dụng `pad_sequences` để đảm bảo chuỗi có độ dài tối đa được chỉ định. Hàm này sẽ cắt ngắn chuỗi nếu nó lớn hơn hoặc đệm bằng số 0 nếu nó ngắn hơn.\n",
    "- **Thiết lập câu trả lời:**\n",
    "- Token hóa câu trả lời ban đầu, lưu ý tất cả các câu trả lời đều bắt đầu bằng chuỗi “answer:”.\n",
    "- Thêm một chiều bổ sung cho tensor để tương thích.\n",
    "- Lấy id của token `EOS`, thường được biểu thị bằng 1.\n",
    "- **Tạo câu trả lời:**\n",
    "- Vòng lặp cho các lần lặp `decode_maxlen`.\n",
    "- Sử dụng hàm `transformer_utils.next_word` để dự đoán mã thông báo tiếp theo trong câu trả lời bằng cách sử dụng mô hình, tài liệu đầu vào và trạng thái hiện tại của đầu ra.\n",
    "- Nối từ tiếp theo được dự đoán với tensor đầu ra.\n",
    "- **Điều kiện dừng:**\n",
    "- Quá trình tạo văn bản sẽ dừng nếu mô hình dự đoán mã thông báo `EOS`.\n",
    "- Nếu dự đoán được mã thông báo `EOS`, hãy thoát khỏi vòng lặp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91def253",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: answer_question\n",
    "def answer_question(question, model, tokenizer, encoder_maxlen=150, decoder_maxlen=50):\n",
    "    \"\"\"\n",
    "    A function for question answering using the transformer model\n",
    "    Arguments:\n",
    "        question (tf.Tensor): Input data with question and context\n",
    "        model (tf.keras.model): The transformer model\n",
    "        tokenizer (function): The SentencePiece tokenizer\n",
    "        encoder_maxlen (number): Max length of the encoded sequence\n",
    "        decoder_maxlen (number): Max length of the decoded sequence\n",
    "    Returns:\n",
    "        _ (str): The answer to the question\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # QUESTION SETUP\n",
    "    \n",
    "    # Tokenize the question\n",
    "    tokenized_question = tokenizer.tokenize(question)\n",
    "    \n",
    "    # Add an extra dimension to the tensor\n",
    "    tokenized_question = tf.expand_dims(tokenized_question, 0) \n",
    "    \n",
    "    # Pad the question tensor\n",
    "    padded_question = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question,\n",
    "                                                                    maxlen=encoder_maxlen,\n",
    "                                                                    padding='post', \n",
    "                                                                    truncating='post') \n",
    "    # ANSWER SETUP\n",
    "    \n",
    "    # Tokenize the answer\n",
    "    # Hint: All answers begin with the string \"answer: \"\n",
    "    tokenized_answer = tokenizer.tokenize('answer: ')\n",
    "    \n",
    "    # Add an extra dimension to the tensor\n",
    "    tokenized_answer = tf.expand_dims(tokenized_answer, 0)\n",
    "    \n",
    "    # Get the id of the EOS token\n",
    "    eos = tokenizer.string_to_id(\"</s>\") \n",
    "    \n",
    "    # Loop for decoder_maxlen iterations\n",
    "    for i in range(decoder_maxlen):\n",
    "        \n",
    "        # Predict the next word using the model, the input document and the current state of output\n",
    "        next_word = transformer_utils.next_word(padded_question, tokenized_answer, model)\n",
    "        \n",
    "        # Concat the predicted next word to the output \n",
    "        tokenized_answer = tf.concat([tokenized_answer, next_word], axis=1)\n",
    "        \n",
    "        # The text generation stops if the model predicts the EOS token\n",
    "        if next_word.numpy()[0][0] == eos:\n",
    "            break \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tokenized_answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de501d8c",
   "metadata": {},
   "source": [
    "Hãy kiểm tra mô hình với một số câu hỏi từ tập dữ liệu huấn luyện. Kiểm tra xem các câu trả lời có khớp với câu trả lời đúng không."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "163e79eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mb'answer: January 9, 1957'\u001b[0m\n",
      "\n",
      "question: When was the Chechen-Ingush Autonomous Soviet Socialist Republic transferred from the Georgian SSR? context: On January 9, 1957, Karachay Autonomous Oblast and Chechen-Ingush Autonomous Soviet Socialist Republic were restored by Khrushchev and they were transferred from the Georgian SSR back to the Russian SFSR.\n",
      "\u001b[32manswer: January 9, 1957\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "idx = 10408\n",
    "\n",
    "result = answer_question(inputs_train[idx], transformer, tokenizer)\n",
    "print(colored(pretty_decode(result, sentinels, tokenizer).numpy()[0], 'blue'))\n",
    "print()\n",
    "print(inputs_train[idx])\n",
    "print(colored(targets_train[idx], 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae9e8d",
   "metadata": {},
   "source": [
    "#### **Đầu ra dự kiến:**\n",
    "```\n",
    "b'answer: January 9, 1957'\n",
    "\n",
    "question: When was the Chechen-Ingush Autonomous Soviet Socialist Republic transferred from the Georgian SSR? context: On January 9, 1957, Karachay Autonomous Oblast and Chechen-Ingush Autonomous Soviet Socialist Republic were restored by Khrushchev and they were transferred from the Georgian SSR back to the Russian SFSR.\n",
    "answer: January 9, 1957\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19ac8067",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "w3_unittest.test_answer_question(answer_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06588341",
   "metadata": {},
   "source": [
    "Kiểm tra mô hình với câu hỏi 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c381df3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mb'answer: 50'\u001b[0m\n",
      "\n",
      "question:  What percentage of the vote was recorded as approving Napoleon's constitution? context: Napoleon established a political system that historian Martyn Lyons called \"dictatorship by plebiscite.\" Worried by the democratic forces unleashed by the Revolution, but unwilling to ignore them entirely, Napoleon resorted to regular electoral consultations with the French people on his road to imperial power. He drafted the Constitution of the Year VIII and secured his own election as First Consul, taking up residence at the Tuileries. The constitution was approved in a rigged plebiscite held the following January, with 99.94 percent officially listed as voting \"yes.\" Napoleon's brother, Lucien, had falsified the returns to show that 3 million people had participated in the plebiscite; the real number was 1.5 million. Political observers at the time assumed the eligible French voting public numbered about 5 million people, so the regime artificially doubled the participation rate to indicate popular enthusiasm for the Consulate. In the first few months of the Consulate, with war in Europe still raging and internal instability still plaguing the country, Napoleon's grip on power remained very tenuous.\n",
      "\u001b[32manswer: 99.94\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "idx = 110\n",
    "result = answer_question(inputs_test[idx], transformer, tokenizer)\n",
    "print(colored(pretty_decode(result, sentinels, tokenizer).numpy()[0], 'blue'))\n",
    "print()\n",
    "print(inputs_test[idx])\n",
    "print(colored(targets_test[idx], 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09ec41",
   "metadata": {},
   "source": [
    "Kiểm tra mô hình với câu hỏi 301. Sử dụng ô này để thử nghiệm mô hình bằng cách chọn các câu hỏi kiểm tra khác. Hãy xem liệu mô hình đã học được điều gì hay nó chỉ tạo ra văn bản ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc9c898f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mb'answer: June 1840'\u001b[0m\n",
      "\n",
      "question:  On what date was a state funeral held for Napoleon? context: In 1840, Louis Philippe I obtained permission from the British to return Napoleon's remains to France. On 15 December 1840, a state funeral was held. The hearse proceeded from the Arc de Triomphe down the Champs-Élysées, across the Place de la Concorde to the Esplanade des Invalides and then to the cupola in St Jérôme's Chapel, where it remained until the tomb designed by Louis Visconti was completed. In 1861, Napoleon's remains were entombed in a porphyry sarcophagus in the crypt under the dome at Les Invalides.\n",
      "\u001b[32manswer: 15 December 1840\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "idx = 311\n",
    "result = answer_question(inputs_test[idx], transformer, tokenizer)\n",
    "print(colored(pretty_decode(result, sentinels, tokenizer).numpy()[0], 'blue'))\n",
    "print()\n",
    "print(inputs_test[idx])\n",
    "print(colored(targets_test[idx], 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d00de",
   "metadata": {},
   "source": [
    "Xin chúc mừng, bạn đã hoàn thành nhiệm vụ cuối cùng của chuyên ngành này. Bây giờ, bạn đã biết điều gì đằng sau những mô hình mạnh mẽ như ChatGPT. Bây giờ là lúc để bạn tìm và giải quyết số lượng lớn các vấn đề có thể gặp phải với NLP."
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
